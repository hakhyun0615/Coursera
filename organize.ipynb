{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4) :   a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.zeros(4,) :  a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.random.random_sample(4): a = [0.22657227 0.54141137 0.53050087 0.7931722 ], a shape = (4,), a data type = float64\n",
      "np.arange(4.):     a = [0. 1. 2. 3.], a shape = (4,), a data type = float64\n",
      "np.random.rand(4): a = [0.82546165 0.49381691 0.87118799 0.4452837 ], a shape = (4,), a data type = float64\n",
      "np.array([5,4,3,2]):  a = [5 4 3 2],     a shape = (4,), a data type = int64\n",
      "np.array([5.,4,3,2]): a = [5. 4. 3. 2.], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(4);                print(f\"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.zeros((4,));             print(f\"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.random_sample(4); print(f\"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.arange(4.);              print(f\"np.arange(4.):     a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.rand(4);          print(f\"np.random.rand(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5,4,3,2]);  print(f\"np.array([5,4,3,2]):  a = {a},     a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5.,4,3,2]); print(f\"np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a             : [1 2 3 4]\n",
      "b = a+3       : [4 5 6 7]\n",
      "b = -a        : [-1 -2 -3 -4]\n",
      "b = np.sum(a) : 10\n",
      "b = np.mean(a): 2.5\n",
      "b = np.exp(a) : [ 2.71828183  7.3890561  20.08553692 54.59815003]\n",
      "b = a**2      : [ 1  4  9 16]\n",
      "b = sigmoid(a): [0.73105858 0.88079708 0.95257413 0.98201379]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "print(f\"a             : {a}\")\n",
    "\n",
    "b = a+3 \n",
    "print(f\"b = a+3       : {b}\")\n",
    "b = -a \n",
    "print(f\"b = -a        : {b}\")\n",
    "b = np.sum(a) \n",
    "print(f\"b = np.sum(a) : {b}\")\n",
    "b = np.mean(a)\n",
    "print(f\"b = np.mean(a): {b}\")\n",
    "b = np.exp(a)\n",
    "print(f\"b = np.exp(a) : {b}\")\n",
    "b = a**2\n",
    "print(f\"b = a**2      : {b}\")\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1+np.exp(-x))\n",
    "    return s\n",
    "b = sigmoid(a)\n",
    "print(f\"b = sigmoid(a): {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([ 1, 2, 3, 4])\n",
    "b = np.array([-1,-2, 3, 4])\n",
    "print(f\"Binary operators work element wise: {a + b}\")\n",
    "\n",
    "c = np.array([1, 2])\n",
    "try:\n",
    "    d = a + c\n",
    "except Exception as e:\n",
    "    print(\"The error message you'll see is:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot/inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a \\cdot b = a^T b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1-D np.dot(a, b) = 24, np.dot(a, b).shape = () \n",
      "NumPy 1-D np.dot(b, a) = 24, np.dot(a, b).shape = () \n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "c = np.dot(a, b)\n",
    "print(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \") \n",
    "c = np.dot(b, a)\n",
    "print(f\"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((1, 5))                                       \n",
    "print(f\"a shape = {a.shape}, a = {a}\")                     \n",
    "a = np.zeros((2, 1))                                                                   \n",
    "print(f\"a shape = {a.shape}, a = {a}\") \n",
    "a = np.random.random_sample((1, 1))  \n",
    "print(f\"a shape = {a.shape}, a = {a}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6).reshape(-1, 2) \n",
    "print(f\"a.shape: {a.shape}, \\na= {a}\")\n",
    "print(f\"a[2,0].shape:   {a[2, 0].shape}, a[2,0] = {a[2, 0]},     type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\")\n",
    "print(f\"a[2].shape:   {a[2].shape}, a[2]   = {a[2]}, type(a[2])   = {type(a[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b shape: (1, 3), \n",
      "a + b = [[6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3]).reshape(1,-1)  # (1,3)\n",
    "b = 5\n",
    "print(f\"a + b shape: {(a + b).shape}, \\na + b = {a + b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b shape: (3, 4), \n",
      "a + b = \n",
      "[[2 3 4 5]\n",
      " [3 4 5 6]\n",
      " [4 5 6 7]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4]).reshape(1,-1) # (1,4)\n",
    "b = np.array([1,2,3]).reshape(-1,1) # (3,1)\n",
    "print(f\"a + b shape: {(a + b).shape}, \\na + b = \\n{a + b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2)\n",
      "(18, 1)\n"
     ]
    }
   ],
   "source": [
    "# image2vector\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                     [ 0.90714982,  0.52835647],\n",
    "                     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                   [[ 0.92814219,  0.96677647],\n",
    "                    [ 0.85304703,  0.52351845],\n",
    "                    [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                   [[ 0.60659855,  0.00533165],\n",
    "                    [ 0.10820313,  0.49978937],\n",
    "                    [ 0.34144279,  0.94630077]]]) # (length, height, 3)\n",
    "vector = image.reshape(-1,1) # (length*height*3, 1)\n",
    "print(image.shape)\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### normalize rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = \\begin{bmatrix}\n",
    "        0 & 3 & 4 \\\\\n",
    "        2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}$$ \n",
    "$$\\| x\\| = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 4]\n",
      " [2 6 4]]\n",
      "[[5.        ]\n",
      " [7.48331477]]\n",
      "[[0.         0.6        0.8       ]\n",
      " [0.26726124 0.80178373 0.53452248]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 3, 4],\n",
    "              [2, 6, 4]])\n",
    "print(x)\n",
    "x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True) # axis=1: row-wise\n",
    "print(x_norm)\n",
    "x = x / x_norm\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}$$\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n",
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "def L1(yhat, y):\n",
    "    loss = np.sum(np.abs(y-yhat))\n",
    "    return loss\n",
    "def L2(yhat, y):\n",
    "    loss = np.sum(np.square(y-yhat))\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))\n",
    "print(\"L2 = \" + str(L2(yhat, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abstraction of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = \n",
    "\\left(\\begin{array}{cc} \n",
    "--- x^{(1)} --- \\\\\n",
    "--- x^{(2)} --- \\\\\n",
    "\\vdots \\\\ \n",
    "--- x^{(m)} --- \n",
    "\\end{array}\\right)\n",
    "Y = \n",
    "\\left(\\begin{array}{cc} \n",
    " y^{(1)}  \\\\\n",
    " y^{(2)}  \\\\\n",
    "\\vdots \\\\ \n",
    " y^{(m)}  \n",
    "\\end{array}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of x_train:\", type(x_train))\n",
    "print ('The first element of X is: ', x_train[0])\n",
    "print(\"First five elements of x_train are:\\n\", x_train[:5]) \n",
    "\n",
    "print(\"Type of y_train:\", type(y_train))\n",
    "print ('The first element of y is: ', y_train[0,0])\n",
    "print(\"First five elements of y_train are:\\n\", y_train[:5])  \n",
    "\n",
    "print ('The shape of x_train is:', x_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show classes in data set\n",
    "print(f\"unique classes {np.unique(y_train)}\")\n",
    "# show how classes are represented\n",
    "print(f\"class representation {y_train[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scatter plot: indictation of which features have the strongest influence on target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\")\n",
    "ax.legend( fontsize='xx-large')\n",
    "ax.set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')\n",
    "ax.set_xlabel('Size (1000 sqft)', fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i], y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(x, y, title):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "    plt.rcParams[\"lines.markersize\"] = 12\n",
    "    plt.scatter(x, y, marker='x', c='r')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\") \n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(x=x, y=y, title=\"input vs. target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = Y_train == 1\n",
    "neg = Y_train == 0\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
    "ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n",
    "              edgecolors=dlc[\"dlblue\"],lw=3)\n",
    "\n",
    "ax.set_ylim(-0.08,1.1)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_title('one variable plot')\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bc_dataset(x, y, title):\n",
    "    for i in range(len(y)):\n",
    "        marker = 'x' if y[i] == 1 else 'o'\n",
    "        c = 'r' if y[i] == 1 else 'b'\n",
    "        plt.scatter(x[i,0], x[i,1], marker=marker, c=c); \n",
    "    plt.title(\"x1 vs x2\")\n",
    "    plt.xlabel(\"x1\"); \n",
    "    plt.ylabel(\"x2\"); \n",
    "    y_0 = mlines.Line2D([], [], color='r', marker='x', markersize=12, linestyle='None', label='y=1')\n",
    "    y_1 = mlines.Line2D([], [], color='b', marker='o', markersize=12, linestyle='None', label='y=0')\n",
    "    plt.title(title)\n",
    "    plt.legend(handles=[y_0, y_1])\n",
    "    plt.show()\n",
    "\n",
    "plot_bc_dataset(x=x_bc, y=y_bc, title=\"x1 vs. x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature engineering (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using domain knowledge to design new features by transforming a feature or combining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add polynomial features    \n",
    "\n",
    "<img src='images/polynomial features.png' width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_cv_mses(degrees, train_mses, cv_mses, title):\n",
    "    degrees = range(1,11)\n",
    "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize lists to save the errors, models, and feature transforms\n",
    "train_mses = []\n",
    "cv_mses = []\n",
    "models = []\n",
    "polys = []\n",
    "scalers = []\n",
    "\n",
    "# Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "for degree in range(1,11):\n",
    "    \n",
    "    # Add polynomial features to the training set\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_train_mapped = poly.fit_transform(x_train)\n",
    "    polys.append(poly)\n",
    "    \n",
    "    # Scale the training set\n",
    "    scaler_poly = StandardScaler()\n",
    "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "    scalers.append(scaler_poly)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_mapped_scaled, y_train )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Compute the training MSE\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    train_mses.append(train_mse)\n",
    "    \n",
    "    # Add polynomial features and scale the cross validation set\n",
    "    X_cv_mapped = poly.transform(x_cv)\n",
    "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "    \n",
    "    # Compute the cross validation MSE\n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    cv_mses.append(cv_mse)\n",
    "    \n",
    "# Plot the results\n",
    "degrees=range(1,11)\n",
    "plot_train_cv_mses(degrees, train_mses, cv_mses, title=\"degree of polynomial vs. train and CV MSEs\")\n",
    "\n",
    "# Get the model with the lowest CV MSE (add 1 because list indices start at 0)\n",
    "# This also corresponds to the degree of the polynomial added\n",
    "degree = np.argmin(cv_mses) + 1\n",
    "print(f\"Lowest CV MSE is found in the model with degree={degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "    degrees = range(1,max_degree+1)\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for degree in degrees:\n",
    "\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model.fit(X_train_mapped_scaled, y_train )\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "        cv_mses.append(cv_mse)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
    "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
    "    plt.xticks(degrees)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorical variable with n outputs &rightarrow; n binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data = df,\n",
    "                    prefix = cat_variables,\n",
    "                    columns = cat_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set: used to train the model  \n",
    "cross validation set: used to tune different model configurations and differentiate between underfitting and overfitting\n",
    "test set: used to represent new data and estimate chosen model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# training set: 60, cross validation set: 20, test set: 20\n",
    "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title):\n",
    "    plt.scatter(x_train, y_train, marker='x', c='r', label='training'); \n",
    "    plt.scatter(x_cv, y_cv, marker='o', c='b', label='cross validation'); \n",
    "    plt.scatter(x_test, y_test, marker='^', c='g', label='test'); \n",
    "    plt.title(\"input vs. target\")\n",
    "    plt.xlabel(\"x\"); \n",
    "    plt.ylabel(\"y\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title=\"input vs. target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "ax.set_title(\"Training, CV, Test\",fontsize = 14)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "ax.scatter(X_cv, y_cv,       color = dlc[\"dlorange\"], label=\"cv\")\n",
    "ax.scatter(X_test, y_test,   color = dlc[\"dlblue\"],   label=\"test\")\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. some labeled anomalous examples\n",
    "    * training set: 0.6 * normal\n",
    "    * cross validation set: 0.2 * normal + 0.5 * anomalous &rightarrow; tune $x_i,\\epsilon$\n",
    "    * test set: 0.2 * normal + 0.5 * anomalous\n",
    "2. few labeled anomalous examples (higher risk of overfitting)\n",
    "    * training set: 0.6 * normal\n",
    "    * cross validation set: 0.4 * normal + anomalous &rightarrow; tune $x_i,\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make feature have similar range(-1~1) to speed up gradient descent\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "$$z \\sim N(0,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train_mapped_scaled,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_train_mapped_scaled,axis=0)}\")\n",
    "\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scalers.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(x, y, title):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "    plt.rcParams[\"lines.markersize\"] = 12\n",
    "    plt.scatter(x, y, marker='x', c='r')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\") \n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(x=X_norm, y=y_train, title=\"scaled input vs. target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\n",
    "print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\n",
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(X)  # learns mean, variance\n",
    "Xn = norm_l(X)\n",
    "print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\n",
    "print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measures the cost of the cross validation set to early stop, automatically control estimators(number of rounds) if the cost on the cross validation set stops to decrease for a number of early_stopping_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
    "xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"Number of fitted trees which achieved the minimum value of the log-loss: {xgb_model.best_iteration}\")\n",
    "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_test),y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f_{\\mathbf{w},b}(x^{(i)}) = \\mathbf{w}\\cdot x^{(i)} + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to minimize a cost function in order to get optimal parameters\n",
    "1. Normal Equation: $w^* = (X^T X)^{-1} X^T Y$\n",
    "2. Gradient Descent: $w := w - \\alpha (X^T (Xw - Y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Closed-Form Solution\n",
    "- 데이터 전체를 한 번에 사용하여 mathmatically Normal Equation을 통해 가중치를 업데이트\n",
    "- 작은 데이터셋에서 사용\n",
    "- Normalization 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train) # X: 2d Matrix, y: 1d vector\n",
    "\n",
    "b = linear_model.intercept_\n",
    "w = linear_model.coef_\n",
    "print(f\"w = {w:}, b = {b:0.2f}\")\n",
    "\n",
    "yhat = linear_model.predict(X_train)\n",
    "print(f\"Prediction on training set:\\n {yhat}\" )\n",
    "print(f\"Target values \\n {y_train}\")\n",
    "print(f\"Training MSE: {mean_squared_error(y_train, yhat) / 2}\")\n",
    "\n",
    "yhat = linear_model.predict(X_cv_norm)\n",
    "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")\n",
    "\n",
    "yhat = models[degree-1].predict(X_test_mapped_scaled)\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, yhat) / 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stochastic Gradient Descent\n",
    "- 한 번에 하나의 샘플을 사용하여 Gradient Descent를 사용하여 가중치를 업데이트\n",
    "- 큰 데이터셋에서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y_train) # X: 2d Matrix, y: 1d vector\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")\n",
    "print(f\"model parameters: w: {sgdr.intercept_}, b:{sgdr.coef_}\")\n",
    "\n",
    "y_pred_sgd = sgdr.predict(X_norm)\n",
    "print(f\"prediction using sgdr.predict: {y_pred_sgd}\")\n",
    "\n",
    "print(f\"Prediction on training set:\\n{y_pred_sgd[:4]}\" )\n",
    "print(f\"Target values \\n{y_train[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions and targets vs original features    \n",
    "fig,ax=plt.subplots(1,4,figsize=(12,3),sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train, label = 'target')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:,i],y_pred,color=dlc[\"dlorange\"], label = 'predict')\n",
    "ax[0].set_ylabel(\"Price\"); ax[0].legend()\n",
    "fig.suptitle(\"target versus prediction using z-score normalized model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions over data range \n",
    "\n",
    "def plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "\n",
    "    ax.set_title(\"Poor Performance on Test Data\",fontsize = 12)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "    ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "    ax.scatter(X_test, y_test,       color = dlc[\"dlblue\"], label=\"test\")\n",
    "    ax.set_xlim(ax.get_xlim())\n",
    "    ax.set_ylim(ax.get_ylim())\n",
    "    ax.plot(x, y_pred,  lw=0.5, label=f\"predicted, degree={degree}\")\n",
    "    ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "x = np.linspace(0,int(X.max()),100)  # predict values for plot\n",
    "y_pred = lmodel.predict(x).reshape(-1,1)\n",
    "\n",
    "plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "- one neuron\n",
    "- no activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "linear_layer = tf.keras.layers.Dense(units=1, activation='linear')\n",
    "a1 = linear_layer(X_train[0].reshape(1,1))\n",
    "print(a1)\n",
    "\n",
    "w, b= linear_layer.get_weights()\n",
    "print(f\"w = {w}, b={b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression: linear regression + sigmoid function\n",
    "\n",
    "$$ 0 \\leq f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\leq 1 $$\n",
    "$$ \\text{if } \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b \\geq 0 \\text{ or } f_{\\mathbf{w},b}(x) \\geq 0.5, \\text{ predict } y=1 $$\n",
    "$$ \\text{if } \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b < 0 \\text{ or } f_{\\mathbf{w},b}(x) < 0.5, \\text{ predict } y=0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "y_pred = lr_model.predict(X)\n",
    "print(\"Prediction on training set:\", y_pred)\n",
    "\n",
    "print(\"Accuracy on training set:\", lr_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(w, b, X, y):\n",
    "    # Credit to dibgerge on Github for this plotting code\n",
    "     \n",
    "    plot_data(X[:, 0:2], y)\n",
    "    \n",
    "    if X.shape[1] <= 2:\n",
    "        plot_x = np.array([min(X[:, 0]), max(X[:, 0])])\n",
    "        plot_y = (-1. / w[1]) * (w[0] * plot_x + b)\n",
    "        \n",
    "        plt.plot(plot_x, plot_y, c=\"b\")\n",
    "        \n",
    "    else:\n",
    "        u = np.linspace(-1, 1.5, 50)\n",
    "        v = np.linspace(-1, 1.5, 50)\n",
    "        \n",
    "        z = np.zeros((len(u), len(v)))\n",
    "\n",
    "        # Evaluate z = theta*x over the grid\n",
    "        for i in range(len(u)):\n",
    "            for j in range(len(v)):\n",
    "                z[i,j] = sig(np.dot(map_feature(u[i], v[j]), w) + b)\n",
    "        \n",
    "        # important to transpose z before calling contour       \n",
    "        z = z.T\n",
    "        \n",
    "        # Plot z = 0.5\n",
    "        plt.contour(u,v,z, levels = [0.5], colors=\"g\")\n",
    "\n",
    "plot_decision_boundary(w, b, X_mapped, y_train)\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Microchip Test 2') \n",
    "# Set the x-axis label\n",
    "plt.xlabel('Microchip Test 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "- one neuron\n",
    "- sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')\n",
    "])\n",
    "print(model.summary())\n",
    "\n",
    "logistic_layer = model.get_layer('L1')\n",
    "w,b = logistic_layer.get_weights()\n",
    "print(w,b)\n",
    "print(w.shape,b.shape)\n",
    "\n",
    "set_w = np.array([[2]])\n",
    "set_b = np.array([-4.5])\n",
    "logistic_layer.set_weights([set_w, set_b])\n",
    "print(logistic_layer.get_weights())\n",
    "\n",
    "a1 = model.predict(X_train[0].reshape(1,1))\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neural network(supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural networks can learn non-linear relationships so you can opt to skip adding polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# to achieve consistent results\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential\n",
    "- straight line\n",
    "- 1 input &rightarrow; list of layers &rightarrow; 1 output  \n",
    "\n",
    "Functional\n",
    "- flexible graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/convolution video.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "$$n_H = \\Bigl\\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_W = \\Bigl\\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_C = \\text{number of filters used in the convolution}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- zero padding\n",
    "    - preserve height, width after convolution(same convolution)\n",
    "    - keep more information at the border of an image  \n",
    "    <img src=\"images/padding.png\" style=\"width:600px;height:400px;\">\n",
    "- convolution\n",
    "    - extract features\n",
    "    - change volume    \n",
    "    <img src=\"images/convolution.gif\" style=\"width:500px;height:300px;\">\n",
    "- pooling\n",
    "    - reduce information while maintaining features\n",
    "    - reduce width,height\n",
    "    - max pooling layer  \n",
    "    <img src=\"images/max pooling.png\" style=\"width:500px;height:300px;\">\n",
    "    - average pooling layer  \n",
    "    <img src=\"images/average pooling.png\" style=\"width:500px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.ZeroPadding2D(padding=(3,3), input_shape=(64,64,3)),\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=(7,7), strides=1),\n",
    "        tf.keras.layers.BatchNormalization(axis=3), # speed up training\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_model(input_shape):\n",
    "    input_img = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    Z1 = tf.keras.layers.Conv2D(filters=8, kernel_size=(4,4), strides=1, padding=\"same\")(input_img)\n",
    "    A1 = tf.keras.layers.ReLU()(Z1)\n",
    "    P1 = tf.keras.layers.MaxPool2D(pool_size=(8,8), strides=8, padding='same')(A1)\n",
    "    Z2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=1, padding=\"same\")(P1)\n",
    "    A2 = tf.keras.layers.ReLU()(Z2)\n",
    "    P2 = tf.keras.layers.MaxPool2D(pool_size=(4,4), strides=4, padding='same')(A2)\n",
    "    F = tf.keras.layers.Flatten()(P2)\n",
    "    outputs = tf.keras.layers.Dense(6, activation=\"softmax\")(F)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_img, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = convolutional_model((64, 64, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './horse-or-human/',  # This is the source directory for training images\n",
    "        target_size=(300, 300),  # All images will be resized to 300x300 # more convolution needed so better results compared to 150x150 which need less convolution\n",
    "        batch_size=128,\n",
    "        # Since you use binary_crossentropy loss, you need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 128 using validation_datagen generator\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        './validation-horse-or-human/',  # This is the source directory for validation images\n",
    "        target_size=(300, 300),  # All images will be resized to 300x300\n",
    "        batch_size=32,\n",
    "        # Since you use binary_crossentropy loss, you need binary labels\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image augmentation\n",
    "- doesn't increase the amount of data rather modifies the existing data dynamically during training\n",
    "- avoid overfitting by generating more scenarios for features in the images\n",
    "- if the validation set doesn't have the same randomness as augmented training set, the results of validation accuracy can fluctuate &rightarrow; so broad set of images for validation and test set is also needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without image generator\n",
    "def data_augmenter():\n",
    "    data_augmentation = tf.keras.Sequential()\n",
    "    data_augmentation.add(tf.keras.layers.RandomFlip(\"horizontal\"))\n",
    "    data_augmentation.add(tf.keras.layers.RandomRotation(0.2))\n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with image generator\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8,\n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " zero_padding2d_4 (ZeroPadd  (None, 70, 70, 3)            0         ['input_6[0][0]']             \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 64)           9472      ['zero_padding2d_4[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 64)           256       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 32, 32, 64)           0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPoolin  (None, 15, 15, 64)           0         ['activation_1[0][0]']        \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 15, 15, 64)           4160      ['max_pooling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 15, 15, 64)           256       ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 15, 15, 64)           0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 15, 15, 64)           36928     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 15, 15, 64)           256       ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 15, 15, 64)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 15, 15, 256)          16640     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 15, 15, 256)          16640     ['max_pooling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 15, 15, 256)          1024      ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 15, 15, 256)          1024      ['conv2d_10[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 15, 15, 256)          0         ['batch_normalization_5[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 15, 15, 256)          0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 15, 15, 64)           16448     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 15, 15, 64)           256       ['conv2d_11[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 15, 15, 64)           0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 15, 15, 64)           36928     ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 15, 15, 64)           256       ['conv2d_12[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 15, 15, 64)           0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 15, 15, 256)          16640     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 15, 15, 256)          1024      ['conv2d_13[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 15, 15, 256)          0         ['batch_normalization_9[0][0]'\n",
      "                                                                    , 'activation_4[0][0]']       \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 15, 15, 256)          0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 15, 15, 64)           16448     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 15, 15, 64)           256       ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 15, 15, 64)           0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 15, 15, 64)           36928     ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 15, 15, 64)           256       ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 15, 15, 64)           0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 15, 15, 256)          16640     ['activation_9[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 15, 15, 256)          1024      ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 15, 15, 256)          0         ['batch_normalization_12[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 15, 15, 256)          0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 8, 8, 128)            32896     ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 8, 8, 128)            512       ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 8, 8, 128)            0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 8, 8, 128)            147584    ['activation_11[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 8, 8, 128)            512       ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 8, 8, 128)            0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 8, 8, 512)            66048     ['activation_12[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 8, 8, 512)            131584    ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 8, 8, 512)            2048      ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 8, 8, 512)            2048      ['conv2d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 8, 8, 512)            0         ['batch_normalization_15[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 8, 8, 512)            0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 8, 8, 128)            65664     ['activation_13[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 8, 8, 128)            512       ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 8, 8, 128)            0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 8, 8, 128)            147584    ['activation_14[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 8, 8, 128)            512       ['conv2d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 8, 8, 128)            0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 8, 8, 512)            66048     ['activation_15[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 8, 8, 512)            2048      ['conv2d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 8, 8, 512)            0         ['batch_normalization_19[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_13[0][0]']       \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, 8, 8, 512)            0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 8, 8, 128)            65664     ['activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 8, 8, 128)            512       ['conv2d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_17 (Activation)  (None, 8, 8, 128)            0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 8, 8, 128)            147584    ['activation_17[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 8, 8, 128)            512       ['conv2d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_18 (Activation)  (None, 8, 8, 128)            0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 8, 8, 512)            66048     ['activation_18[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 8, 8, 512)            2048      ['conv2d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 8, 8, 512)            0         ['batch_normalization_22[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " activation_19 (Activation)  (None, 8, 8, 512)            0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 8, 8, 128)            65664     ['activation_19[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 8, 8, 128)            512       ['conv2d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_20 (Activation)  (None, 8, 8, 128)            0         ['batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 8, 8, 128)            147584    ['activation_20[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 8, 8, 128)            512       ['conv2d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_21 (Activation)  (None, 8, 8, 128)            0         ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 8, 8, 512)            66048     ['activation_21[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 8, 8, 512)            2048      ['conv2d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 8, 8, 512)            0         ['batch_normalization_25[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_19[0][0]']       \n",
      "                                                                                                  \n",
      " activation_22 (Activation)  (None, 8, 8, 512)            0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 4, 4, 256)            131328    ['activation_22[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_26 (Ba  (None, 4, 4, 256)            1024      ['conv2d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_23 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_26[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 4, 4, 256)            590080    ['activation_23[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_27 (Ba  (None, 4, 4, 256)            1024      ['conv2d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_24 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_27[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 4, 4, 1024)           263168    ['activation_24[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 4, 4, 1024)           525312    ['activation_22[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_28 (Ba  (None, 4, 4, 1024)           4096      ['conv2d_32[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_29 (Ba  (None, 4, 4, 1024)           4096      ['conv2d_33[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 4, 4, 1024)           0         ['batch_normalization_28[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_29[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_25 (Activation)  (None, 4, 4, 1024)           0         ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 4, 4, 256)            262400    ['activation_25[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_30 (Ba  (None, 4, 4, 256)            1024      ['conv2d_34[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_26 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_30[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 4, 4, 256)            590080    ['activation_26[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_31 (Ba  (None, 4, 4, 256)            1024      ['conv2d_35[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_27 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_31[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 4, 4, 1024)           263168    ['activation_27[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_32 (Ba  (None, 4, 4, 1024)           4096      ['conv2d_36[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 4, 4, 1024)           0         ['batch_normalization_32[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_25[0][0]']       \n",
      "                                                                                                  \n",
      " activation_28 (Activation)  (None, 4, 4, 1024)           0         ['add_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 4, 4, 256)            262400    ['activation_28[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_33 (Ba  (None, 4, 4, 256)            1024      ['conv2d_37[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_29 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)          (None, 4, 4, 256)            590080    ['activation_29[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_34 (Ba  (None, 4, 4, 256)            1024      ['conv2d_38[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_30 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)          (None, 4, 4, 1024)           263168    ['activation_30[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_35 (Ba  (None, 4, 4, 1024)           4096      ['conv2d_39[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 4, 4, 1024)           0         ['batch_normalization_35[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_28[0][0]']       \n",
      "                                                                                                  \n",
      " activation_31 (Activation)  (None, 4, 4, 1024)           0         ['add_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)          (None, 4, 4, 256)            262400    ['activation_31[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_36 (Ba  (None, 4, 4, 256)            1024      ['conv2d_40[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_32 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)          (None, 4, 4, 256)            590080    ['activation_32[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_37 (Ba  (None, 4, 4, 256)            1024      ['conv2d_41[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_33 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)          (None, 4, 4, 1024)           263168    ['activation_33[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_38 (Ba  (None, 4, 4, 1024)           4096      ['conv2d_42[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 4, 4, 1024)           0         ['batch_normalization_38[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_31[0][0]']       \n",
      "                                                                                                  \n",
      " activation_34 (Activation)  (None, 4, 4, 1024)           0         ['add_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)          (None, 4, 4, 256)            262400    ['activation_34[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_39 (Ba  (None, 4, 4, 256)            1024      ['conv2d_43[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_35 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)          (None, 4, 4, 256)            590080    ['activation_35[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_40 (Ba  (None, 4, 4, 256)            1024      ['conv2d_44[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_36 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)          (None, 4, 4, 1024)           263168    ['activation_36[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_41 (Ba  (None, 4, 4, 1024)           4096      ['conv2d_45[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 4, 4, 1024)           0         ['batch_normalization_41[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_34[0][0]']       \n",
      "                                                                                                  \n",
      " activation_37 (Activation)  (None, 4, 4, 1024)           0         ['add_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)          (None, 4, 4, 256)            262400    ['activation_37[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_42 (Ba  (None, 4, 4, 256)            1024      ['conv2d_46[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_38 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)          (None, 4, 4, 256)            590080    ['activation_38[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_43 (Ba  (None, 4, 4, 256)            1024      ['conv2d_47[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_39 (Activation)  (None, 4, 4, 256)            0         ['batch_normalization_43[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)          (None, 4, 4, 1024)           263168    ['activation_39[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 4, 4, 1024)           4096      ['conv2d_48[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 4, 4, 1024)           0         ['batch_normalization_44[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_37[0][0]']       \n",
      "                                                                                                  \n",
      " activation_40 (Activation)  (None, 4, 4, 1024)           0         ['add_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)          (None, 2, 2, 512)            524800    ['activation_40[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_45 (Ba  (None, 2, 2, 512)            2048      ['conv2d_49[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_41 (Activation)  (None, 2, 2, 512)            0         ['batch_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)          (None, 2, 2, 512)            2359808   ['activation_41[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_46 (Ba  (None, 2, 2, 512)            2048      ['conv2d_50[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_42 (Activation)  (None, 2, 2, 512)            0         ['batch_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)          (None, 2, 2, 2048)           1050624   ['activation_42[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)          (None, 2, 2, 2048)           2099200   ['activation_40[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_47 (Ba  (None, 2, 2, 2048)           8192      ['conv2d_51[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_48 (Ba  (None, 2, 2, 2048)           8192      ['conv2d_52[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 2, 2, 2048)           0         ['batch_normalization_47[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_48[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_43 (Activation)  (None, 2, 2, 2048)           0         ['add_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)          (None, 2, 2, 512)            1049088   ['activation_43[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_49 (Ba  (None, 2, 2, 512)            2048      ['conv2d_53[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_44 (Activation)  (None, 2, 2, 512)            0         ['batch_normalization_49[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)          (None, 2, 2, 512)            2359808   ['activation_44[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_50 (Ba  (None, 2, 2, 512)            2048      ['conv2d_54[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_45 (Activation)  (None, 2, 2, 512)            0         ['batch_normalization_50[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)          (None, 2, 2, 2048)           1050624   ['activation_45[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_51 (Ba  (None, 2, 2, 2048)           8192      ['conv2d_55[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 2, 2, 2048)           0         ['batch_normalization_51[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_43[0][0]']       \n",
      "                                                                                                  \n",
      " activation_46 (Activation)  (None, 2, 2, 2048)           0         ['add_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)          (None, 2, 2, 512)            1049088   ['activation_46[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_52 (Ba  (None, 2, 2, 512)            2048      ['conv2d_56[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_47 (Activation)  (None, 2, 2, 512)            0         ['batch_normalization_52[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)          (None, 2, 2, 512)            2359808   ['activation_47[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_53 (Ba  (None, 2, 2, 512)            2048      ['conv2d_57[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_48 (Activation)  (None, 2, 2, 512)            0         ['batch_normalization_53[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)          (None, 2, 2, 2048)           1050624   ['activation_48[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_54 (Ba  (None, 2, 2, 2048)           8192      ['conv2d_58[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 2, 2, 2048)           0         ['batch_normalization_54[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'activation_46[0][0]']       \n",
      "                                                                                                  \n",
      " activation_49 (Activation)  (None, 2, 2, 2048)           0         ['add_15[0][0]']              \n",
      "                                                                                                  \n",
      " average_pooling2d (Average  (None, 1, 1, 2048)           0         ['activation_49[0][0]']       \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)         (None, 2048)                 0         ['average_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 6)                    12294     ['flatten_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23600006 (90.03 MB)\n",
      "Trainable params: 23546886 (89.82 MB)\n",
      "Non-trainable params: 53120 (207.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ matrix: $(u_{in}, u_{out})$  \n",
    "$b$ vector: ($u_{out},)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1(2, 3):\n",
      " [[-0.62252975  0.38249552 -0.13287777]\n",
      " [ 0.09647369  1.009989   -1.0684577 ]] \n",
      "b1(3,): [0. 0. 0.]\n",
      "W2(3, 1):\n",
      " [[ 1.1811253 ]\n",
      " [-0.50675476]\n",
      " [-0.5505224 ]] \n",
      "b2(1,): [0.]\n"
     ]
    }
   ],
   "source": [
    "# instantiated weights and biases\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights() # model.layers[0].get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights() # model.layers[1].get_weights()\n",
    "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
    "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function: 하나의 input data에 대해 target과 오차를 계산하는 함수  \n",
    "cost function: 모든 input data에 대해 losses를 계산하는 함수로 학습을 통해 이를 최소화 ex) MSE, Binary Cross Entropy, Cross Entropy\n",
    "\n",
    "$$ cost function = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} loss function$$  \n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} L(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE: \\frac{1}{m} \\sum_{i=0}^{m-1} \\frac{1}{2}(y_i - \\hat{y}_i)^2$$  \n",
    "$$Binary Cross Entropy: -\\frac{1}{m} \\sum_{i=0}^{m-1} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$  \n",
    "$$Cross Entropy: -\\frac{1}{m} \\sum_{i=0}^{m-1} y_i \\log(\\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent: $x \\rightarrow x - \\alpha \\nabla f(x)$ 를 통해 cost function을 최소화\n",
    "\n",
    "$$\\begin{align*}& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Gradient Descent\n",
    "- update parameters for entire training dataset\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "m = X.shape[1]  # Number of training examples\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Forward propagation\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    # Compute cost\n",
    "    cost_total = compute_cost(a, Y)  # Cost for m training examples\n",
    "    # Backward propagation\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost\n",
    "    cost_avg = cost_total / m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch Gradient Descent\n",
    "- update parameters for n training examples\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "m = X.shape[1]  # Number of training examples\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    cost_total = 0\n",
    "    for (minibatch_X, minibatch_Y) in minibatches:\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(minibatch_X, parameters)   \n",
    "        # Compute cost\n",
    "        cost_total += compute_cost(a, minibatch_Y)\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost (optional)\n",
    "    cost_avg = cost_total / mini_batch_size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "- update parameters for 1 training example\n",
    "- fast → used when training set is large\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "m = X.shape[1]  # Number of training examples\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    cost_total = 0\n",
    "    for j in range(0, m):\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Compute cost\n",
    "        cost_total += compute_cost(a, Y[:,j])  # Cost for one training example\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost\n",
    "    cost_avg = cost_total / m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate decay: redue learning rate over time to converge to the minimun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "model.compile(\n",
    "    # loss = 'mse' # regression\n",
    "    # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), # binary classification(0/1)\n",
    "    # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # multiclass classification(0~9)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent(efficiency broken into 32 batches)\n",
    "history = model.fit(\n",
    "    Xt,Yt,            \n",
    "    epochs=10,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated weights and biases\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
    "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track the progress of gradient descent\n",
    "def widgvis(fig):\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "\n",
    "def plot_loss_tf(history):\n",
    "    fig,ax = plt.subplots(1,1, figsize = (4,3))\n",
    "    widgvis(fig)\n",
    "    ax.plot(history.history['loss'], label='loss')\n",
    "    ax.set_ylim([0, 2])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('loss (cost)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_tf(history)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "acc = [0.] + history.history['accuracy']\n",
    "val_acc = [0.] + history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "######################################################################\n",
    "\n",
    "df_loss_acc = pd.DataFrame(history.history)\n",
    "df_loss= df_loss_acc[['loss','val_loss']]\n",
    "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
    "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
    "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
    "df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\n",
    "df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with loss and metrics\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "X_testn = norm_l(X_test)\n",
    "X_testn = tf.convert_to_tensor(X_testn)\n",
    "\n",
    "first_prediction = model.predict(X_testn[0].reshape(1,400)) \n",
    "print(f\"first prediction: {first_prediction}\")\n",
    "predictions = model.predict(X_testn)\n",
    "print(\"predictions = \\n\", predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification class conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_yhat = (predictions >= 0.5).astype(int) # binary classification(convert probabilites to decision)\n",
    "\n",
    "mc_yhat = tf.nn.softmax(predictions.numpy()) # multiclass classfication(if the desired output is probability)\n",
    "mc_yhat = np.argmax(predictions, axis=1) # multiclass classfication(if the desired output is category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cat_err(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the categorization error\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:|\n",
    "      err: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    incorrect = 0\n",
    "    for i in range(m):\n",
    "        if yhat[i] != y[i]:\n",
    "            incorrect += 1\n",
    "    err = incorrect/m\n",
    "    return(err)\n",
    "\n",
    "training_cerr_complex = eval_cat_err(y_train, model_predict(X_train))\n",
    "cv_cerr_complex = eval_cat_err(y_cv, model_predict(X_cv))\n",
    "print(f\"categorization error, training, complex model: {training_cerr_complex:0.3f}\")\n",
    "print(f\"categorization error, cv,       complex model: {cv_cerr_complex:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = lambda Xl: np.argmax(tf.nn.softmax(model.predict(Xl)).numpy(),axis=1)\n",
    "plt_nn(model_predict,X_train,y_train, classes, X_cv, y_cv, suptitle=\"Complex Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_224\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)       [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)              (None, 112, 112, 32)         864       ['input_16[0][0]']            \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalizati  (None, 112, 112, 32)         128       ['Conv1[0][0]']               \n",
      " on)                                                                                              \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)           (None, 112, 112, 32)         0         ['bn_Conv1[0][0]']            \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (D  (None, 112, 112, 32)         288       ['Conv1_relu[0][0]']          \n",
      " epthwiseConv2D)                                                                                  \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN  (None, 112, 112, 32)         128       ['expanded_conv_depthwise[0][0\n",
      "  (BatchNormalization)                                              ]']                           \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_re  (None, 112, 112, 32)         0         ['expanded_conv_depthwise_BN[0\n",
      " lu (ReLU)                                                          ][0]']                        \n",
      "                                                                                                  \n",
      " expanded_conv_project (Con  (None, 112, 112, 16)         512       ['expanded_conv_depthwise_relu\n",
      " v2D)                                                               [0][0]']                      \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (  (None, 112, 112, 16)         64        ['expanded_conv_project[0][0]'\n",
      " BatchNormalization)                                                ]                             \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)     (None, 112, 112, 96)         1536      ['expanded_conv_project_BN[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNo  (None, 112, 112, 96)         384       ['block_1_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)  (None, 112, 112, 96)         0         ['block_1_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D  (None, 113, 113, 96)         0         ['block_1_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_1_depthwise (Depthwi  (None, 56, 56, 96)           864       ['block_1_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (Batc  (None, 56, 56, 96)           384       ['block_1_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (Re  (None, 56, 56, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)    (None, 56, 56, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchN  (None, 56, 56, 24)           96        ['block_1_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)     (None, 56, 56, 144)          3456      ['block_1_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNo  (None, 56, 56, 144)          576       ['block_2_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)  (None, 56, 56, 144)          0         ['block_2_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_depthwise (Depthwi  (None, 56, 56, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (Batc  (None, 56, 56, 144)          576       ['block_2_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (Re  (None, 56, 56, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)    (None, 56, 56, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchN  (None, 56, 56, 24)           96        ['block_2_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_2_add (Add)           (None, 56, 56, 24)           0         ['block_1_project_BN[0][0]',  \n",
      "                                                                     'block_2_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)     (None, 56, 56, 144)          3456      ['block_2_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNo  (None, 56, 56, 144)          576       ['block_3_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)  (None, 56, 56, 144)          0         ['block_3_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D  (None, 57, 57, 144)          0         ['block_3_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_3_depthwise (Depthwi  (None, 28, 28, 144)          1296      ['block_3_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (Batc  (None, 28, 28, 144)          576       ['block_3_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (Re  (None, 28, 28, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)    (None, 28, 28, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_3_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_3_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_4_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_4_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_depthwise (Depthwi  (None, 28, 28, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (Batc  (None, 28, 28, 192)          768       ['block_4_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (Re  (None, 28, 28, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)    (None, 28, 28, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_4_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_4_add (Add)           (None, 28, 28, 32)           0         ['block_3_project_BN[0][0]',  \n",
      "                                                                     'block_4_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_4_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_5_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_5_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_depthwise (Depthwi  (None, 28, 28, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (Batc  (None, 28, 28, 192)          768       ['block_5_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (Re  (None, 28, 28, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)    (None, 28, 28, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_5_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_5_add (Add)           (None, 28, 28, 32)           0         ['block_4_add[0][0]',         \n",
      "                                                                     'block_5_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_5_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_6_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_6_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D  (None, 29, 29, 192)          0         ['block_6_expand_relu[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_6_depthwise (Depthwi  (None, 14, 14, 192)          1728      ['block_6_pad[0][0]']         \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (Batc  (None, 14, 14, 192)          768       ['block_6_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (Re  (None, 14, 14, 192)          0         ['block_6_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)    (None, 14, 14, 64)           12288     ['block_6_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_6_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_6_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_7_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_7_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_7_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_7_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (Re  (None, 14, 14, 384)          0         ['block_7_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_7_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_7_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_7_add (Add)           (None, 14, 14, 64)           0         ['block_6_project_BN[0][0]',  \n",
      "                                                                     'block_7_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_7_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_8_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_8_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_8_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_8_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (Re  (None, 14, 14, 384)          0         ['block_8_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_8_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_8_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_8_add (Add)           (None, 14, 14, 64)           0         ['block_7_add[0][0]',         \n",
      "                                                                     'block_8_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_8_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_9_expand[0][0]']      \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_9_expand_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_9_expand_relu[0][0]'] \n",
      " seConv2D)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_9_depthwise[0][0]']   \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (Re  (None, 14, 14, 384)          0         ['block_9_depthwise_BN[0][0]']\n",
      " LU)                                                                                              \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_9_depthwise_relu[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_9_project[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_9_add (Add)           (None, 14, 14, 64)           0         ['block_8_add[0][0]',         \n",
      "                                                                     'block_9_project_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)    (None, 14, 14, 384)          24576     ['block_9_add[0][0]']         \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchN  (None, 14, 14, 384)          1536      ['block_10_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU  (None, 14, 14, 384)          0         ['block_10_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_10_depthwise (Depthw  (None, 14, 14, 384)          3456      ['block_10_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (Bat  (None, 14, 14, 384)          1536      ['block_10_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (R  (None, 14, 14, 384)          0         ['block_10_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)   (None, 14, 14, 96)           36864     ['block_10_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_10_project_BN (Batch  (None, 14, 14, 96)           384       ['block_10_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)    (None, 14, 14, 576)          55296     ['block_10_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchN  (None, 14, 14, 576)          2304      ['block_11_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU  (None, 14, 14, 576)          0         ['block_11_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_11_depthwise (Depthw  (None, 14, 14, 576)          5184      ['block_11_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (Bat  (None, 14, 14, 576)          2304      ['block_11_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (R  (None, 14, 14, 576)          0         ['block_11_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)   (None, 14, 14, 96)           55296     ['block_11_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_11_project_BN (Batch  (None, 14, 14, 96)           384       ['block_11_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_11_add (Add)          (None, 14, 14, 96)           0         ['block_10_project_BN[0][0]', \n",
      "                                                                     'block_11_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)    (None, 14, 14, 576)          55296     ['block_11_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchN  (None, 14, 14, 576)          2304      ['block_12_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU  (None, 14, 14, 576)          0         ['block_12_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_12_depthwise (Depthw  (None, 14, 14, 576)          5184      ['block_12_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (Bat  (None, 14, 14, 576)          2304      ['block_12_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (R  (None, 14, 14, 576)          0         ['block_12_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)   (None, 14, 14, 96)           55296     ['block_12_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_12_project_BN (Batch  (None, 14, 14, 96)           384       ['block_12_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_12_add (Add)          (None, 14, 14, 96)           0         ['block_11_add[0][0]',        \n",
      "                                                                     'block_12_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)    (None, 14, 14, 576)          55296     ['block_12_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchN  (None, 14, 14, 576)          2304      ['block_13_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU  (None, 14, 14, 576)          0         ['block_13_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2  (None, 15, 15, 576)          0         ['block_13_expand_relu[0][0]']\n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " block_13_depthwise (Depthw  (None, 7, 7, 576)            5184      ['block_13_pad[0][0]']        \n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (Bat  (None, 7, 7, 576)            2304      ['block_13_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (R  (None, 7, 7, 576)            0         ['block_13_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)   (None, 7, 7, 160)            92160     ['block_13_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_13_project_BN (Batch  (None, 7, 7, 160)            640       ['block_13_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)    (None, 7, 7, 960)            153600    ['block_13_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchN  (None, 7, 7, 960)            3840      ['block_14_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU  (None, 7, 7, 960)            0         ['block_14_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_14_depthwise (Depthw  (None, 7, 7, 960)            8640      ['block_14_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (Bat  (None, 7, 7, 960)            3840      ['block_14_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (R  (None, 7, 7, 960)            0         ['block_14_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)   (None, 7, 7, 160)            153600    ['block_14_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_14_project_BN (Batch  (None, 7, 7, 160)            640       ['block_14_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_14_add (Add)          (None, 7, 7, 160)            0         ['block_13_project_BN[0][0]', \n",
      "                                                                     'block_14_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)    (None, 7, 7, 960)            153600    ['block_14_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchN  (None, 7, 7, 960)            3840      ['block_15_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU  (None, 7, 7, 960)            0         ['block_15_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_15_depthwise (Depthw  (None, 7, 7, 960)            8640      ['block_15_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (Bat  (None, 7, 7, 960)            3840      ['block_15_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (R  (None, 7, 7, 960)            0         ['block_15_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)   (None, 7, 7, 160)            153600    ['block_15_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_15_project_BN (Batch  (None, 7, 7, 160)            640       ['block_15_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " block_15_add (Add)          (None, 7, 7, 160)            0         ['block_14_add[0][0]',        \n",
      "                                                                     'block_15_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)    (None, 7, 7, 960)            153600    ['block_15_add[0][0]']        \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchN  (None, 7, 7, 960)            3840      ['block_16_expand[0][0]']     \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU  (None, 7, 7, 960)            0         ['block_16_expand_BN[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " block_16_depthwise (Depthw  (None, 7, 7, 960)            8640      ['block_16_expand_relu[0][0]']\n",
      " iseConv2D)                                                                                       \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (Bat  (None, 7, 7, 960)            3840      ['block_16_depthwise[0][0]']  \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (R  (None, 7, 7, 960)            0         ['block_16_depthwise_BN[0][0]'\n",
      " eLU)                                                               ]                             \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)   (None, 7, 7, 320)            307200    ['block_16_depthwise_relu[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " block_16_project_BN (Batch  (None, 7, 7, 320)            1280      ['block_16_project[0][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)             (None, 7, 7, 1280)           409600    ['block_16_project_BN[0][0]'] \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalizat  (None, 7, 7, 1280)           5120      ['Conv_1[0][0]']              \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " out_relu (ReLU)             (None, 7, 7, 1280)           0         ['Conv_1_bn[0][0]']           \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4  (None, 1280)                 0         ['out_relu[0][0]']            \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " predictions (Dense)         (None, 1000)                 1281000   ['global_average_pooling2d_4[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3538984 (13.50 MB)\n",
      "Trainable params: 3504872 (13.37 MB)\n",
      "Non-trainable params: 34112 (133.25 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "global_average_pooling2d_4\n",
      "predictions\n"
     ]
    }
   ],
   "source": [
    "def transfer_model(image_shape, data_augmentation=data_augmenter()):\n",
    "    # load the pretrained model weights\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                include_top=True,\n",
    "                                                weights='imagenet') # pretrained weights from ImageNet\n",
    "\n",
    "    # print the summary of the model\n",
    "    print(base_model.summary())\n",
    "\n",
    "    # print the top layers\n",
    "    nb_layers = len(base_model.layers)\n",
    "    print(base_model.layers[nb_layers - 2].name)\n",
    "    print(base_model.layers[nb_layers - 1].name)\n",
    "\n",
    "    # delete the top layers\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                    include_top=False, # delete\n",
    "                                                    weights='imagenet')\n",
    "\n",
    "    # freeze the base model by making it non trainable\n",
    "    base_model.trainable = False \n",
    "\n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=IMG_SHAPE) \n",
    "\n",
    "    # apply data augmentation to the inputs\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # data preprocessing using the same normalization standard\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x) \n",
    "\n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x, training=False) \n",
    "\n",
    "    # add the new Binary classification layers\n",
    "    # use global avg pooling to summarize the info in each channel\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x) \n",
    "    # include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "    # use a prediction layer with one neuron (as a binary classifier only needs one)\n",
    "    prediction_layer = tf.keras.layers.Dense(1)\n",
    "    outputs = prediction_layer(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = transfer_model((224,224,3))\n",
    "\n",
    "base_learning_rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "initial_epochs = 5\n",
    "history = model.fit(train_dataset, validation_data=validation_dataset, epochs=initial_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  154\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m loss_function\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mbase_learning_rate\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Use accuracy as evaluation metric\u001b[39;00m\n\u001b[1;32m     20\u001b[0m metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "base_model = model.layers[4]\n",
    "base_model.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 120\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Define a BinaryCrossentropy loss function. Use from_logits=True\n",
    "loss_function=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "# Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate*0.1)\n",
    "# Use accuracy as evaluation metric\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer = optimizer,\n",
    "              metrics=metrics)\n",
    "\n",
    "fine_tune_epochs = 5\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)\n",
    "\n",
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = tf.keras.models.load_model('resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pre_trained_model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the training parameters at the end of each epoch  \n",
    "if it meets the threshold, stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    '''\n",
    "    Halts the training when the loss falls below 0.4\n",
    "\n",
    "    Args:\n",
    "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "      logs (dict) - metric results from the training epoch\n",
    "    '''\n",
    "\n",
    "    # Check the loss\n",
    "    if (logs.get('loss') < 0.4):\n",
    "\n",
    "      # Stop if threshold is met\n",
    "      print(\"\\nLoss is lower than 0.4 so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "# Instantiate class\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop if val_loss does not enhance for 2 epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights if val_loss enhances(Early Stopping으로 훈련이 중단된 시점이 아니라, 검증 성능이 가장 좋았던 시점의 모델 가중치를 사용 가능)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.h5',  # 저장할 파일 이름\n",
    "    save_best_only=True,  # 최적의 모델만 저장\n",
    "    monitor='val_loss',  # 모니터링할 지표\n",
    "    mode='min',  # 'min'은 val_loss를 모니터링할 때 사용\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    callbacks=[callbacks, early_stopping, model_checkpoint], \n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speed up gradient descent  \n",
    "avoid vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bias/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the baseline level of performance(human performance)  \n",
    "therefore, consider the gap between training error and cross validation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{just right: } J_{\\text{train}} \\downarrow, J_{\\text{cv}} \\downarrow$  \n",
    "\n",
    "$\\text{high bias problem/underfitting: } J_{\\text{train}} \\uparrow, J_{\\text{cv}} \\uparrow/{\\text{ training loss is far wrose than the baseline level of performance}}$\n",
    "* try adding polynomial features\n",
    "* try getting additional features\n",
    "* try decreasing the regularization parameter  \n",
    "\n",
    "$\\text{high variance problem/overfitting: } J_{\\text{train}} \\downarrow, J_{\\text{cv}} \\uparrow$\n",
    "* try increasing the regularization parameter\n",
    "* try smaller sets of features\n",
    "* get more training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 regularization\n",
    "* generalize to not learn complex patterns in data\n",
    "* encourages gradient descent to minimize $\\mathbf{w}$\n",
    "* used only in training\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{1}{2}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{linear regression} $$ \n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{logistic regression} $$\n",
    "$$\\begin{align*} \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\\\ \\end{align*}$$\n",
    "$$\\begin{align*}&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}   \\; & \\text{for j := 0..n-1} \\\\ \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r = Sequential(\n",
    "    [\n",
    "        Dense(units=120, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "        Dense(units=40, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "        Dense(units=6, activation='linear'),\n",
    "    ], name= None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout\n",
    "* in each iteration randomly shut down some neruons for both forward and backward propatations\n",
    "* able to not rely on specific neurons\n",
    "* used only in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models():\n",
    "    \n",
    "    tf.random.set_seed(20)\n",
    "    \n",
    "    model_1 = Sequential(\n",
    "        [\n",
    "            Dense(25, activation = 'relu'),\n",
    "            Dense(15, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_1'\n",
    "    )\n",
    "\n",
    "    model_2 = Sequential(\n",
    "        [\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_2'\n",
    "    )\n",
    "\n",
    "    model_3 = Sequential(\n",
    "        [\n",
    "            Dense(32, activation = 'relu'),\n",
    "            Dense(16, activation = 'relu'),\n",
    "            Dense(8, activation = 'relu'),\n",
    "            Dense(4, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_3'\n",
    "    )\n",
    "    \n",
    "    model_list = [model_1, model_2, model_3]\n",
    "    \n",
    "    return model_list\n",
    "\n",
    "\n",
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_mses = []\n",
    "nn_cv_mses = []\n",
    "\n",
    "# Build the models\n",
    "nn_models = build_models()\n",
    "\n",
    "# Loop over the the models\n",
    "for model in nn_models:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_mapped_scaled, y_train,\n",
    "        epochs=300,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    \n",
    "    # Record the training MSEs\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    nn_train_mses.append(train_mse)\n",
    "    \n",
    "    # Record the cross validation MSEs \n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    nn_cv_mses.append(cv_mse)\n",
    "\n",
    "    \n",
    "# print results\n",
    "print(\"RESULTS:\")\n",
    "for model_num in range(len(nn_train_mses)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
    "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
    "        )\n",
    "    \n",
    "# Select the model with the lowest CV MSE\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification models(error metric: fraction of the data that the model has misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_error = []\n",
    "nn_cv_error = []\n",
    "\n",
    "# Build the models\n",
    "models_bc = utils.build_models()\n",
    "\n",
    "# Loop over each model\n",
    "for model in models_bc:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_bc_train_scaled, y_bc_train,\n",
    "        epochs=200,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "    \n",
    "    # Set the threshold for classification\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Record the fraction of misclassified examples for the training set\n",
    "    yhat = model.predict(x_bc_train_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    train_error = np.mean(yhat != y_bc_train)\n",
    "    nn_train_error.append(train_error)\n",
    "\n",
    "    # Record the fraction of misclassified examples for the cross validation set\n",
    "    yhat = model.predict(x_bc_cv_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    cv_error = np.mean(yhat != y_bc_cv)\n",
    "    nn_cv_error.append(cv_error)\n",
    "\n",
    "# Print the result\n",
    "for model_num in range(len(nn_train_error)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
    "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest error\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test error\n",
    "yhat = models_bc[model_num-1].predict(x_bc_test_scaled)\n",
    "yhat = tf.math.sigmoid(yhat)\n",
    "yhat = np.where(yhat >= threshold, 1, 0)\n",
    "nn_test_error = np.mean(yhat != y_bc_test)\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\n",
    "print(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\n",
    "print(f\"Test Set Classification Error: {nn_test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for reg_param in reg_params:\n",
    "\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model = Ridge(alpha=reg_param)\n",
    "        model.fit(X_train_mapped_scaled, y_train)\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "        cv_mses.append(cv_mse)\n",
    "\n",
    "    # Plot the results\n",
    "    reg_params = [str(x) for x in reg_params]\n",
    "    plt.plot(reg_params, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(reg_params, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(reg_params, np.repeat(baseline, len(reg_params)), linestyle='--', label='baseline')\n",
    "    plt.title(\"lambda vs. train and CV MSEs\")\n",
    "    plt.xlabel(\"lambda\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define lambdas to plot\n",
    "reg_params = [10, 5, 2, 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01]\n",
    "\n",
    "# Define degree of polynomial and train for each value of lambda\n",
    "train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_range = np.array([0.0, 1e-6, 1e-5, 1e-4,1e-3,1e-2, 1e-1,1,10,100])\n",
    "num_steps = len(lambda_range)\n",
    "degree = 10\n",
    "err_train = np.zeros(num_steps)    \n",
    "err_cv = np.zeros(num_steps)       \n",
    "x = np.linspace(0,int(X.max()),100) \n",
    "y_pred = np.zeros((100,num_steps))  #columns are lines to plot\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lambda_= lambda_range[i]\n",
    "    lmodel = lin_model(degree, regularization=True, lambda_=lambda_)\n",
    "    lmodel.fit(X_train, y_train)\n",
    "    yhat = lmodel.predict(X_train)\n",
    "    err_train[i] = lmodel.mse(y_train, yhat)\n",
    "    yhat = lmodel.predict(X_cv)\n",
    "    err_cv[i] = lmodel.mse(y_cv, yhat)\n",
    "    y_pred[:,i] = lmodel.predict(x)\n",
    "    \n",
    "optimal_reg_idx = np.argmin(err_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "models=[None] * len(lambdas)\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    lambda_ = lambdas[i]\n",
    "    models[i] =  Sequential(\n",
    "        [\n",
    "            Dense(120, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "            Dense(40, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "            Dense(classes, activation = 'linear')\n",
    "        ]\n",
    "    )\n",
    "    models[i].compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "    )\n",
    "\n",
    "    models[i].fit(\n",
    "        X_train,y_train,\n",
    "        epochs=1000\n",
    "    )\n",
    "    print(f\"Finished lambda = {lambda_}\")\n",
    "\n",
    "def plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv):\n",
    "    err_train = np.zeros(len(lambdas))\n",
    "    err_cv = np.zeros(len(lambdas))\n",
    "    for i in range(len(models)):\n",
    "        err_train[i] = eval_cat_err(y_train,np.argmax( models[i](X_train), axis=1))\n",
    "        err_cv[i] = eval_cat_err(y_cv, np.argmax( models[i](X_cv), axis=1))\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "    ax.set_title(\"error vs regularization\",fontsize = 12)\n",
    "    ax.plot(lambdas, err_train, marker='o', label=\"train error\", color = dlc[\"dlblue\"])\n",
    "    ax.plot(lambdas, err_cv,    marker='o', label=\"cv error\",    color = dlc[\"dlorange\"])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(*ax.get_ylim())\n",
    "    ax.set_xlabel(\"Regularization (lambda)\",fontsize = 14)\n",
    "    ax.set_ylabel(\"Error\",fontsize = 14)\n",
    "    ax.legend()\n",
    "    fig.suptitle(\"Tuning Regularization\",fontsize = 14)\n",
    "    ax.text(0.05,0.14,\"Training Error\\nlower than CV\",fontsize=12, ha='left',transform=ax.transAxes,color = dlc[\"dlblue\"])\n",
    "    ax.text(0.95,0.14,\"Similar\\nTraining, CV\",    fontsize=12, ha='right',transform=ax.transAxes,color = dlc[\"dlblue\"])\n",
    "    plt.show()\n",
    "\n",
    "plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neural network(unsupervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train from normal examples and filter examples that aren't normal \n",
    "* anomaly detection vs supervised learning 1\n",
    "    1. anomaly detection: capable of finding previously unseen defects\n",
    "    2. supervised learning: capable of finding previously seen defects\n",
    "* anomaly detection vs supervised learning 2\n",
    "    1. anomaly detection: needs to choose what features to use\n",
    "    2. supervised learning: don't need to choose what features to use (able to learn which features to not use from labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. draw histogram for each features\n",
    "    $$p(x_i ; \\mu_i,\\sigma_i ^2) \\rightarrow \\mathbf{plt.hist(x)}$$\n",
    "2. make non-gaussian features more like gaussian (don't forget to apply to cross validation, test set)\n",
    "    $$log(x_i)$$\n",
    "    $$log(x_i + C)$$\n",
    "    $$\\sqrt[2]{x_i}$$\n",
    "    $$\\sqrt[3]{x_i}$$\n",
    "2. estimate Gaussian distribution from features ($x_i$ &rightarrow; $\\mu_i$, $\\sigma_i^2$)\n",
    "$$\\mu_i = \\frac{1}{m} \\sum_{j=1}^m x_i^{(j)}$$\n",
    "$$\\sigma_i^2 = \\frac{1}{m} \\sum_{j=1}^m (x_i^{(j)} - \\mu_i)^2$$\n",
    "$$p(x_i ; \\mu_i,\\sigma_i ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^2}}\\exp^{ - \\frac{(x_i - \\mu_i)^2}{2 \\sigma_i ^2} }$$\n",
    "3. identify anomalous example\n",
    "    * want $p(x)$ large for normal examples and small for anomalous examples\n",
    "    * if $p(x) is comparable(large) for normal and anomalous examples, add more features to achieve above\n",
    "$$p(x) = \\prod_{i=1}^{n} p(x_i; \\mu_i, \\sigma_i^2)$$\n",
    "$$if\\ anomaly,\\ p(x) < \\epsilon\\rightarrow y = 1$$\n",
    "4. select best $\\epsilon$ using F1 score\n",
    "$$F_1 = \\frac{2\\cdot prec \\cdot rec}{prec + rec}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate mean and variance of each feature\n",
    "def estimate_gaussian(X): \n",
    "    \"\"\"\n",
    "    Calculates mean and variance of all features \n",
    "    in the dataset\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Data matrix\n",
    "    \n",
    "    Returns:\n",
    "        mu (ndarray): (n,) Mean of all features\n",
    "        var (ndarray): (n,) Variance of all features\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    mu = 1/m * np.sum(X, axis=0)\n",
    "    var = 1/m * np.sum((X-mu)**2, axis=0)\n",
    "        \n",
    "    return mu, var\n",
    "\n",
    "mu, var = estimate_gaussian(X_train)              \n",
    "\n",
    "print(\"Mean of each feature:\", mu)\n",
    "print(\"Variance of each feature:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the probabilites for the training set\n",
    "def multivariate_gaussian(X, mu, var):\n",
    "    \"\"\"\n",
    "    Computes the probability \n",
    "    density function of the examples X under the multivariate gaussian \n",
    "    distribution with parameters mu and var. If var is a matrix, it is\n",
    "    treated as the covariance matrix. If var is a vector, it is treated\n",
    "    as the var values of the variances in each dimension (a diagonal\n",
    "    covariance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    k = len(mu)\n",
    "    \n",
    "    if var.ndim == 1:\n",
    "        var = np.diag(var)\n",
    "        \n",
    "    X = X - mu\n",
    "    p = (2* np.pi)**(-k/2) * np.linalg.det(var)**(-0.5) * \\\n",
    "        np.exp(-0.5 * np.sum(np.matmul(X, np.linalg.pinv(var)) * X, axis=1))\n",
    "    \n",
    "    return p\n",
    "\n",
    "p_train = multivariate_gaussian(X_train, mu, var)\n",
    "p_val = multivariate_gaussian(X_val, mu, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best threshold\n",
    "def select_threshold(y_val, p_val): \n",
    "    \"\"\"\n",
    "    Finds the best threshold to use for selecting outliers \n",
    "    based on the results from a validation set (p_val) \n",
    "    and the ground truth (y_val)\n",
    "    \n",
    "    Args:\n",
    "        y_val (ndarray): Ground truth on validation set\n",
    "        p_val (ndarray): Results on validation set\n",
    "        \n",
    "    Returns:\n",
    "        epsilon (float): Threshold chosen \n",
    "        F1 (float):      F1 score by choosing epsilon as threshold\n",
    "    \"\"\" \n",
    "\n",
    "    best_epsilon = 0\n",
    "    best_F1 = 0\n",
    "    F1 = 0\n",
    "    \n",
    "    step_size = (max(p_val) - min(p_val)) / 1000\n",
    "    \n",
    "    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n",
    "        \n",
    "        predictions = p_val < epsilon\n",
    "\n",
    "        tp = np.sum((predictions == 1) & (y_val == 1))\n",
    "        fp = np.sum((predictions == 1) & (y_val == 0))\n",
    "        fn = np.sum((predictions == 0) & (y_val == 1))\n",
    "\n",
    "        prec = tp / (tp + fp)\n",
    "        rec = tp / (tp + fn)\n",
    "\n",
    "        F1 = 2 * prec * rec / (prec + rec)\n",
    "        \n",
    "        if F1 > best_F1:\n",
    "            best_F1 = F1\n",
    "            best_epsilon = epsilon\n",
    "        \n",
    "    return best_epsilon, best_F1\n",
    "\n",
    "epsilon, F1 = select_threshold(y_val, p_val)\n",
    "\n",
    "print('Best epsilon found using cross-validation: %e' % epsilon)\n",
    "print('Best F1 on Cross Validation Set: %f' % F1)\n",
    "print('# Anomalies found: %d'% sum(p_val < epsilon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
