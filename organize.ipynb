{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "shape 계산시, batch_size 고려 x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(4);                print(f\"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.zeros((4,));             print(f\"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.random_sample(4); print(f\"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.arange(4.);              print(f\"np.arange(4.):     a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.rand(4);          print(f\"np.random.rand(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5,4,3,2]);  print(f\"np.array([5,4,3,2]):  a = {a},     a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5.,4,3,2]); print(f\"np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "print(f\"a             : {a}\")\n",
    "\n",
    "b = a+3 \n",
    "print(f\"b = a+3       : {b}\")\n",
    "b = -a \n",
    "print(f\"b = -a        : {b}\")\n",
    "b = np.sum(a) \n",
    "print(f\"b = np.sum(a) : {b}\")\n",
    "b = np.mean(a)\n",
    "print(f\"b = np.mean(a): {b}\")\n",
    "b = np.exp(a)\n",
    "print(f\"b = np.exp(a) : {b}\")\n",
    "b = a**2\n",
    "print(f\"b = a**2      : {b}\")\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1+np.exp(-x))\n",
    "    return s\n",
    "b = sigmoid(a)\n",
    "print(f\"b = sigmoid(a): {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([ 1, 2, 3, 4])\n",
    "b = np.array([-1,-2, 3, 4])\n",
    "print(f\"Binary operators work element wise: {a + b}\")\n",
    "\n",
    "c = np.array([1, 2])\n",
    "try:\n",
    "    d = a + c\n",
    "except Exception as e:\n",
    "    print(\"The error message you'll see is:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot/inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a \\cdot b = a^T b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "c = np.dot(a, b)\n",
    "print(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \") \n",
    "c = np.dot(b, a)\n",
    "print(f\"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((1, 5))                                       \n",
    "print(f\"a shape = {a.shape}, a = {a}\")                     \n",
    "a = np.zeros((2, 1))                                                                   \n",
    "print(f\"a shape = {a.shape}, a = {a}\") \n",
    "a = np.random.random_sample((1, 1))  \n",
    "print(f\"a shape = {a.shape}, a = {a}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6).reshape(-1, 2) \n",
    "print(f\"a.shape: {a.shape}, \\na= {a}\")\n",
    "print(f\"a[2,0].shape:   {a[2, 0].shape}, a[2,0] = {a[2, 0]},     type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\")\n",
    "print(f\"a[2].shape:   {a[2].shape}, a[2]   = {a[2]}, type(a[2])   = {type(a[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3]).reshape(1,-1)  # (1,3)\n",
    "b = 5\n",
    "print(f\"a + b shape: {(a + b).shape}, \\na + b = {a + b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4]).reshape(1,-1) # (1,4)\n",
    "b = np.array([1,2,3]).reshape(-1,1) # (3,1)\n",
    "print(f\"a + b shape: {(a + b).shape}, \\na + b = \\n{a + b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image2vector\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                     [ 0.90714982,  0.52835647],\n",
    "                     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                   [[ 0.92814219,  0.96677647],\n",
    "                    [ 0.85304703,  0.52351845],\n",
    "                    [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                   [[ 0.60659855,  0.00533165],\n",
    "                    [ 0.10820313,  0.49978937],\n",
    "                    [ 0.34144279,  0.94630077]]]) # (length, height, 3)\n",
    "vector = image.reshape(-1,1) # (length*height*3, 1)\n",
    "print(image.shape)\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### normalize rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = \\begin{bmatrix}\n",
    "        0 & 3 & 4 \\\\\n",
    "        2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}$$ \n",
    "$$\\| x\\| = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0, 3, 4],\n",
    "              [2, 6, 4]])\n",
    "print(x)\n",
    "x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True) # axis=1: row-wise\n",
    "print(x_norm)\n",
    "x = x / x_norm\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}$$\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(yhat, y):\n",
    "    loss = np.sum(np.abs(y-yhat))\n",
    "    return loss\n",
    "def L2(yhat, y):\n",
    "    loss = np.sum(np.square(y-yhat))\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))\n",
    "print(\"L2 = \" + str(L2(yhat, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abstraction of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = \n",
    "\\left(\\begin{array}{cc} \n",
    "--- x^{(1)} --- \\\\\n",
    "--- x^{(2)} --- \\\\\n",
    "\\vdots \\\\ \n",
    "--- x^{(m)} --- \n",
    "\\end{array}\\right)\n",
    "Y = \n",
    "\\left(\\begin{array}{cc} \n",
    " y^{(1)}  \\\\\n",
    " y^{(2)}  \\\\\n",
    "\\vdots \\\\ \n",
    " y^{(m)}  \n",
    "\\end{array}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of x_train:\", type(x_train))\n",
    "print ('The first element of X is: ', x_train[0])\n",
    "print(\"First five elements of x_train are:\\n\", x_train[:5]) \n",
    "\n",
    "print(\"Type of y_train:\", type(y_train))\n",
    "print ('The first element of y is: ', y_train[0,0])\n",
    "print(\"First five elements of y_train are:\\n\", y_train[:5])  \n",
    "\n",
    "print ('The shape of x_train is:', x_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show classes in data set\n",
    "print(f\"unique classes {np.unique(y_train)}\")\n",
    "# show how classes are represented\n",
    "print(f\"class representation {y_train[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scatter plot: indictation of which features have the strongest influence on target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\")\n",
    "ax.legend( fontsize='xx-large')\n",
    "ax.set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')\n",
    "ax.set_xlabel('Size (1000 sqft)', fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i], y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(x, y, title):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "    plt.rcParams[\"lines.markersize\"] = 12\n",
    "    plt.scatter(x, y, marker='x', c='r')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\") \n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(x=x, y=y, title=\"input vs. target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = Y_train == 1\n",
    "neg = Y_train == 0\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
    "ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n",
    "              edgecolors=dlc[\"dlblue\"],lw=3)\n",
    "\n",
    "ax.set_ylim(-0.08,1.1)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_title('one variable plot')\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bc_dataset(x, y, title):\n",
    "    for i in range(len(y)):\n",
    "        marker = 'x' if y[i] == 1 else 'o'\n",
    "        c = 'r' if y[i] == 1 else 'b'\n",
    "        plt.scatter(x[i,0], x[i,1], marker=marker, c=c); \n",
    "    plt.title(\"x1 vs x2\")\n",
    "    plt.xlabel(\"x1\"); \n",
    "    plt.ylabel(\"x2\"); \n",
    "    y_0 = mlines.Line2D([], [], color='r', marker='x', markersize=12, linestyle='None', label='y=1')\n",
    "    y_1 = mlines.Line2D([], [], color='b', marker='o', markersize=12, linestyle='None', label='y=0')\n",
    "    plt.title(title)\n",
    "    plt.legend(handles=[y_0, y_1])\n",
    "    plt.show()\n",
    "\n",
    "plot_bc_dataset(x=x_bc, y=y_bc, title=\"x1 vs. x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature engineering (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using domain knowledge to design new features by transforming a feature or combining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add polynomial features    \n",
    "\n",
    "<img src='images/polynomial features.png' width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_cv_mses(degrees, train_mses, cv_mses, title):\n",
    "    degrees = range(1,11)\n",
    "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize lists to save the errors, models, and feature transforms\n",
    "train_mses = []\n",
    "cv_mses = []\n",
    "models = []\n",
    "polys = []\n",
    "scalers = []\n",
    "\n",
    "# Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "for degree in range(1,11):\n",
    "    \n",
    "    # Add polynomial features to the training set\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_train_mapped = poly.fit_transform(x_train)\n",
    "    polys.append(poly)\n",
    "    \n",
    "    # Scale the training set\n",
    "    scaler_poly = StandardScaler()\n",
    "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "    scalers.append(scaler_poly)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_mapped_scaled, y_train )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Compute the training MSE\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    train_mses.append(train_mse)\n",
    "    \n",
    "    # Add polynomial features and scale the cross validation set\n",
    "    X_cv_mapped = poly.transform(x_cv)\n",
    "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "    \n",
    "    # Compute the cross validation MSE\n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    cv_mses.append(cv_mse)\n",
    "    \n",
    "# Plot the results\n",
    "degrees=range(1,11)\n",
    "plot_train_cv_mses(degrees, train_mses, cv_mses, title=\"degree of polynomial vs. train and CV MSEs\")\n",
    "\n",
    "# Get the model with the lowest CV MSE (add 1 because list indices start at 0)\n",
    "# This also corresponds to the degree of the polynomial added\n",
    "degree = np.argmin(cv_mses) + 1\n",
    "print(f\"Lowest CV MSE is found in the model with degree={degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "    degrees = range(1,max_degree+1)\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for degree in degrees:\n",
    "\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model.fit(X_train_mapped_scaled, y_train )\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "        cv_mses.append(cv_mse)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
    "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
    "    plt.xticks(degrees)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorical variable with n outputs &rightarrow; n binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data = df,\n",
    "                    prefix = cat_variables,\n",
    "                    columns = cat_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set: used to train the model  \n",
    "cross validation set: used to tune different model configurations and differentiate between underfitting and overfitting  \n",
    "test set: used to represent new data and estimate chosen model's performance  \n",
    "\n",
    "cross validation set's distribution should be similar to the test set distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# training set: 60, cross validation set: 20, test set: 20\n",
    "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title):\n",
    "    plt.scatter(x_train, y_train, marker='x', c='r', label='training'); \n",
    "    plt.scatter(x_cv, y_cv, marker='o', c='b', label='cross validation'); \n",
    "    plt.scatter(x_test, y_test, marker='^', c='g', label='test'); \n",
    "    plt.title(\"input vs. target\")\n",
    "    plt.xlabel(\"x\"); \n",
    "    plt.ylabel(\"y\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title=\"input vs. target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "ax.set_title(\"Training, CV, Test\",fontsize = 14)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "ax.scatter(X_cv, y_cv,       color = dlc[\"dlorange\"], label=\"cv\")\n",
    "ax.scatter(X_test, y_test,   color = dlc[\"dlblue\"],   label=\"test\")\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. some labeled anomalous examples\n",
    "    * training set: 0.6 * normal\n",
    "    * cross validation set: 0.2 * normal + 0.5 * anomalous &rightarrow; tune $x_i,\\epsilon$\n",
    "    * test set: 0.2 * normal + 0.5 * anomalous\n",
    "2. few labeled anomalous examples (higher risk of overfitting)\n",
    "    * training set: 0.6 * normal\n",
    "    * cross validation set: 0.4 * normal + anomalous &rightarrow; tune $x_i,\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make feature have similar range(-1~1) to speed up gradient descent\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "$$z \\sim N(0,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train_mapped_scaled,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_train_mapped_scaled,axis=0)}\")\n",
    "\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scalers.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(x, y, title):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "    plt.rcParams[\"lines.markersize\"] = 12\n",
    "    plt.scatter(x, y, marker='x', c='r')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\") \n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(x=X_norm, y=y_train, title=\"scaled input vs. target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\n",
    "print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\n",
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(X)  # learns mean, variance\n",
    "Xn = norm_l(X)\n",
    "print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\n",
    "print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measures the cost of the cross validation set to early stop, automatically control estimators(number of rounds) if the cost on the cross validation set stops to decrease for a number of early_stopping_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
    "xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"Number of fitted trees which achieved the minimum value of the log-loss: {xgb_model.best_iteration}\")\n",
    "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_test),y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f_{\\mathbf{w},b}(x^{(i)}) = \\mathbf{w}\\cdot x^{(i)} + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to minimize a cost function in order to get optimal parameters\n",
    "1. Normal Equation: $w^* = (X^T X)^{-1} X^T Y$\n",
    "2. Gradient Descent: $w := w - \\alpha (X^T (Xw - Y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Closed-Form Solution\n",
    "- 데이터 전체를 한 번에 사용하여 mathmatically Normal Equation을 통해 가중치를 업데이트\n",
    "- 작은 데이터셋에서 사용\n",
    "- Normalization 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train) # X: 2d Matrix, y: 1d vector\n",
    "\n",
    "b = linear_model.intercept_\n",
    "w = linear_model.coef_\n",
    "print(f\"w = {w:}, b = {b:0.2f}\")\n",
    "\n",
    "yhat = linear_model.predict(X_train)\n",
    "print(f\"Prediction on training set:\\n {yhat}\" )\n",
    "print(f\"Target values \\n {y_train}\")\n",
    "print(f\"Training MSE: {mean_squared_error(y_train, yhat) / 2}\")\n",
    "\n",
    "yhat = linear_model.predict(X_cv_norm)\n",
    "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")\n",
    "\n",
    "yhat = models[degree-1].predict(X_test_mapped_scaled)\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, yhat) / 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stochastic Gradient Descent\n",
    "- 한 번에 하나의 샘플을 사용하여 Gradient Descent를 사용하여 가중치를 업데이트\n",
    "- 큰 데이터셋에서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y_train) # X: 2d Matrix, y: 1d vector\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")\n",
    "print(f\"model parameters: w: {sgdr.intercept_}, b:{sgdr.coef_}\")\n",
    "\n",
    "y_pred_sgd = sgdr.predict(X_norm)\n",
    "print(f\"prediction using sgdr.predict: {y_pred_sgd}\")\n",
    "\n",
    "print(f\"Prediction on training set:\\n{y_pred_sgd[:4]}\" )\n",
    "print(f\"Target values \\n{y_train[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions and targets vs original features    \n",
    "fig,ax=plt.subplots(1,4,figsize=(12,3),sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train, label = 'target')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:,i],y_pred,color=dlc[\"dlorange\"], label = 'predict')\n",
    "ax[0].set_ylabel(\"Price\"); ax[0].legend()\n",
    "fig.suptitle(\"target versus prediction using z-score normalized model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions over data range \n",
    "\n",
    "def plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "\n",
    "    ax.set_title(\"Poor Performance on Test Data\",fontsize = 12)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "    ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "    ax.scatter(X_test, y_test,       color = dlc[\"dlblue\"], label=\"test\")\n",
    "    ax.set_xlim(ax.get_xlim())\n",
    "    ax.set_ylim(ax.get_ylim())\n",
    "    ax.plot(x, y_pred,  lw=0.5, label=f\"predicted, degree={degree}\")\n",
    "    ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "x = np.linspace(0,int(X.max()),100)  # predict values for plot\n",
    "y_pred = lmodel.predict(x).reshape(-1,1)\n",
    "\n",
    "plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "- one neuron\n",
    "- no activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "linear_layer = tf.keras.layers.Dense(units=1, activation='linear')\n",
    "a1 = linear_layer(X_train[0].reshape(1,1))\n",
    "print(a1)\n",
    "\n",
    "w, b= linear_layer.get_weights()\n",
    "print(f\"w = {w}, b={b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression: linear regression + sigmoid function\n",
    "\n",
    "$$ 0 \\leq f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\leq 1 $$\n",
    "$$ \\text{if } \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b \\geq 0 \\text{ or } f_{\\mathbf{w},b}(x) \\geq 0.5, \\text{ predict } y=1 $$\n",
    "$$ \\text{if } \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b < 0 \\text{ or } f_{\\mathbf{w},b}(x) < 0.5, \\text{ predict } y=0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "y_pred = lr_model.predict(X)\n",
    "print(\"Prediction on training set:\", y_pred)\n",
    "\n",
    "print(\"Accuracy on training set:\", lr_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(w, b, X, y):\n",
    "    # Credit to dibgerge on Github for this plotting code\n",
    "     \n",
    "    plot_data(X[:, 0:2], y)\n",
    "    \n",
    "    if X.shape[1] <= 2:\n",
    "        plot_x = np.array([min(X[:, 0]), max(X[:, 0])])\n",
    "        plot_y = (-1. / w[1]) * (w[0] * plot_x + b)\n",
    "        \n",
    "        plt.plot(plot_x, plot_y, c=\"b\")\n",
    "        \n",
    "    else:\n",
    "        u = np.linspace(-1, 1.5, 50)\n",
    "        v = np.linspace(-1, 1.5, 50)\n",
    "        \n",
    "        z = np.zeros((len(u), len(v)))\n",
    "\n",
    "        # Evaluate z = theta*x over the grid\n",
    "        for i in range(len(u)):\n",
    "            for j in range(len(v)):\n",
    "                z[i,j] = sig(np.dot(map_feature(u[i], v[j]), w) + b)\n",
    "        \n",
    "        # important to transpose z before calling contour       \n",
    "        z = z.T\n",
    "        \n",
    "        # Plot z = 0.5\n",
    "        plt.contour(u,v,z, levels = [0.5], colors=\"g\")\n",
    "\n",
    "plot_decision_boundary(w, b, X_mapped, y_train)\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Microchip Test 2') \n",
    "# Set the x-axis label\n",
    "plt.xlabel('Microchip Test 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "- one neuron\n",
    "- sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')\n",
    "])\n",
    "print(model.summary())\n",
    "\n",
    "logistic_layer = model.get_layer('L1')\n",
    "w,b = logistic_layer.get_weights()\n",
    "print(w,b)\n",
    "print(w.shape,b.shape)\n",
    "\n",
    "set_w = np.array([[2]])\n",
    "set_b = np.array([-4.5])\n",
    "logistic_layer.set_weights([set_w, set_b])\n",
    "print(logistic_layer.get_weights())\n",
    "\n",
    "a1 = model.predict(X_train[0].reshape(1,1))\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neural network(supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural networks can learn non-linear relationships so you can opt to skip adding polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# to achieve consistent results\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential\n",
    "- straight line\n",
    "- 1 input &rightarrow; list of layers &rightarrow; 1 output  \n",
    "\n",
    "Functional\n",
    "- flexible graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/convolution video.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "$$n_H = \\Bigl\\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_W = \\Bigl\\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_C = \\text{number of filters used in the convolution}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- zero padding\n",
    "    - preserve height, width after convolution(same convolution)\n",
    "    - keep more information at the border of an image  \n",
    "    <img src=\"images/padding.png\" style=\"width:600px;height:400px;\">\n",
    "- convolution\n",
    "    - extract features\n",
    "    - change volume    \n",
    "    <img src=\"images/convolution.gif\" style=\"width:500px;height:300px;\">\n",
    "- pooling\n",
    "    - reduce information while maintaining features\n",
    "    - reduce width,height\n",
    "    - max pooling layer  \n",
    "    <img src=\"images/max pooling.png\" style=\"width:500px;height:300px;\">\n",
    "    - average pooling layer  \n",
    "    <img src=\"images/average pooling.png\" style=\"width:500px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.ZeroPadding2D(padding=(3,3), input_shape=(64,64,3)),\n",
    "        tf.keras.layers.Conv2D(filters=32, kernel_size=(7,7), strides=1),\n",
    "        tf.keras.layers.BatchNormalization(axis=3), # speed up training\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_model(input_shape):\n",
    "    input_img = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    Z1 = tf.keras.layers.Conv2D(filters=8, kernel_size=(4,4), strides=1, padding=\"same\")(input_img)\n",
    "    A1 = tf.keras.layers.ReLU()(Z1)\n",
    "    P1 = tf.keras.layers.MaxPool2D(pool_size=(8,8), strides=8, padding='same')(A1)\n",
    "    Z2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(2,2), strides=1, padding=\"same\")(P1)\n",
    "    A2 = tf.keras.layers.ReLU()(Z2)\n",
    "    P2 = tf.keras.layers.MaxPool2D(pool_size=(4,4), strides=4, padding='same')(A2)\n",
    "    F = tf.keras.layers.Flatten()(P2)\n",
    "    outputs = tf.keras.layers.Dense(6, activation=\"softmax\")(F)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_img, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = convolutional_model((64, 64, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 128 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './horse-or-human/',  # This is the source directory for training images\n",
    "        target_size=(300, 300),  # All images will be resized to 300x300 # more convolution needed so better results compared to 150x150 which need less convolution\n",
    "        batch_size=128,\n",
    "        # Since you use binary_crossentropy loss, you need binary labels\n",
    "        class_mode='binary') # categorical\n",
    "\n",
    "# Flow validation images in batches of 128 using validation_datagen generator\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        './validation-horse-or-human/',  # This is the source directory for validation images\n",
    "        target_size=(300, 300),  # All images will be resized to 300x300\n",
    "        batch_size=32,\n",
    "        # Since you use binary_crossentropy loss, you need binary labels\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(\n",
    "    x=training_images, # numpy array\n",
    "    y=training_labels,\n",
    "    batch_size=32)\n",
    "\n",
    "validation_generator = validation_datagen.flow(\n",
    "    x=validation_images,\n",
    "    y=validation_labels,\n",
    "    batch_size=32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image augmentation\n",
    "- doesn't increase the amount of data rather modifies the existing data dynamically during training\n",
    "- avoid overfitting by generating more scenarios for features in the images\n",
    "- if the validation set doesn't have the same randomness as augmented training set, the results of validation accuracy can fluctuate &rightarrow; so broad set of images for validation and test set is also needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without image generator\n",
    "def data_augmenter():\n",
    "    data_augmentation = tf.keras.Sequential()\n",
    "    data_augmentation.add(tf.keras.layers.RandomFlip(\"horizontal\"))\n",
    "    data_augmentation.add(tf.keras.layers.RandomRotation(0.2))\n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with image generator\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=8,\n",
    "    epochs=15,\n",
    "    verbose=1,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN\n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:500;height:300px;\">  \n",
    "\n",
    "- $m$: minibatch size\n",
    "- $n_x/n_a/n_y$: number of units\n",
    "- $T_x/T_y$: number of timesteps\n",
    "- input $x$: $(n_x, m, T_x)$\n",
    "    - $x^{\\langle t \\rangle}$: $(n_x, m)$, $x^{(i)}$: $(n_x, T_x)$\n",
    "        - $x^{(i) \\langle t \\rangle }$: $(n_x, )$\n",
    "- hidden state $a$: $(n_a, m, T_x)$\n",
    "    - $a^{\\langle t \\rangle}$: $(n_a, m)$\n",
    "- prediction $\\hat{y}$: $(n_{y}, m, T_{y})$\n",
    "    - $\\hat{y}^{\\langle t \\rangle}$: $(n_y, m)$   \n",
    "\n",
    "RNN cell\n",
    "\n",
    "<img src=\"images/rnn cell.png\" style=\"width:700px;height:300px;\">\n",
    "\n",
    "- RNN cell(weights and biases) is reused each time step \n",
    "- inputs: $a^{\\langle t-1 \\rangle}$, $x^{\\langle t \\rangle}$\n",
    "- outputs: $a^{\\langle t \\rangle}$, $y^{\\langle t \\rangle}$\n",
    "    - $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$\n",
    "    - $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$   \n",
    "    \n",
    "RNN performance\n",
    "- vanishing gradients problem\n",
    "- estimates using local context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM cell\n",
    "\n",
    "<img src=\"images/lstm cell.png\" style=\"width:500;height:400px;\">\n",
    "\n",
    "- forget gate \n",
    "    - $\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)$\n",
    "    - $\\mathbf{\\Gamma}_f^{\\langle t \\rangle} * \\mathbf{c}^{\\langle t-1 \\rangle}$\n",
    "        - $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$ is a mask\n",
    "        - if a unit in $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$ ≈ 0, forgets corresponding unit of $\\mathbf{c}^{\\langle t-1 \\rangle}$\n",
    "        - if a unit in $\\mathbf{\\Gamma}_f^{\\langle t \\rangle}$ ≈ 1, keeps corresponding unit of  $\\mathbf{c}^{\\langle t-1 \\rangle}$\n",
    "- update gate\n",
    "    - $\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)$\n",
    "    - $\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} * \\tilde{c}^{\\langle t \\rangle}$\n",
    "        - $\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle}$ decides what parts of a $\\tilde{\\mathbf{c}}^{\\langle t \\rangle}$ are passed onto the $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "            - $\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$ is a candidate value, current information\n",
    "                - $\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right)$\n",
    "            - $\\mathbf{c}^{\\langle t \\rangle}$ is a cell state, memory passed onto future\n",
    "                - $\\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle}$\n",
    "                - $\\mathbf{c}$: $(n_{a}, m, T_x)$, $\\mathbf{c}^{\\langle t \\rangle}$: $(n_{a}, m)$\n",
    "        - if a unit in $\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle}$ ≈ 1, updates corresponding unit of $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "        - if a unit in $\\mathbf{\\Gamma}_{i}^{\\langle t \\rangle}$ ≈ 0, keeps corresponding unit of $\\mathbf{c}^{\\langle t \\rangle}$\n",
    "- output gate\n",
    "    - $\\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})$\n",
    "    - output gate determines $\\mathbf{a}^{\\langle t \\rangle}$, $\\mathbf{y}^{\\langle t \\rangle}_{pred}$\n",
    "        - $\\mathbf{a}^{\\langle t \\rangle}$ is a hidden state\n",
    "            - $\\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})$\n",
    "            - $\\mathbf{a}$: $(n_{a}, m, T_{x})$, $\\mathbf{a}^{\\langle t \\rangle}$: $(n_{a}, m)$ \n",
    "        - $\\mathbf{y}^{\\langle t \\rangle}_{pred}$ is a predeiction (classification in this case)\n",
    "            - $\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$\n",
    "            - $\\mathbf{y}_{pred}$: $(n_{y}, m, T_{y})$, $\\mathbf{y}^{\\langle t \\rangle}_{pred}$: $(n_{y}, m)$\n",
    "\n",
    "LSTM shape\n",
    "\n",
    "1. (기본값) return_sequences=False, return_state=False:\n",
    "   - returns the hidden state of the last timestep\n",
    "   - a = LSTM(x)\n",
    "      - a: (batch_size, num_units)\n",
    "\n",
    "2. return_sequences=True, return_state=False:\n",
    "   - returns the hidden states for each timestep\n",
    "   - a_all = LSTM(x)\n",
    "      - a_all: (batch_size, sequence_length, num_units)\n",
    "\n",
    "3. return_sequences=False, return_state=True:\n",
    "   - returns the last hidden state, the last hidden state, the last cell state\n",
    "   - a, a, c = LSTM(x)\n",
    "      - a: (batch_size, num_units)\n",
    "      - c: (batch_size, num_units)\n",
    "\n",
    "4. return_sequences=True, return_state=True:\n",
    "   - returns the hidden states for each timestep, the last hidden state and the last cell state\n",
    "   - a_all, a, c = LSTM(x)\n",
    "      - a_all: (batch_size, sequence_length, num_units)\n",
    "      - a: (batch_size, num_units)\n",
    "      - c: (batch_size, num_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Step Attention\n",
    "\n",
    "<img src=\"images/one step attention.png\" style=\"width:500;height:500px;\">\n",
    "    \n",
    "1. $a^{\\langle t' \\rangle} = [\\overrightarrow{a}^{\\langle t' \\rangle}, \\overleftarrow{a}^{\\langle t' \\rangle}]$\n",
    "    - original words' hidden state of Pre-attention Bi-LSTM\n",
    "2. $e^{\\langle t, t' \\rangle}$ = Dense($[s^{<t-1>},a^{<t'>}]$) \n",
    "    - how much previous translated word and original word are related\n",
    "    - attention score\n",
    "2. $\\alpha^{<t,t'>}$ = Softmax($e^{\\langle t, t' \\rangle}$)\n",
    "    - turn above relationships into proportions / amount of attention to pay to $a^{\\langle t' \\rangle}$ when creating $y^{\\langle t \\rangle}$\n",
    "    - attention weight\n",
    "3. $context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}$\n",
    "    - weighted sum of $a^{\\langle t' \\rangle}$\n",
    "    \n",
    "Attention\n",
    "\n",
    "- developed for translation\n",
    "\n",
    "<img src=\"images/attention model.png\" style=\"width:500;height:500px;\">   \n",
    "\n",
    "1. $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$ (Pre-attention Bi-LSTM)\n",
    "2. for loop $T_y$ times\n",
    "    - $context^{<t>}$ (One Step Attention)\n",
    "    - $\\hat{y}^{<t>}$ (Post-attention LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    '''\n",
    "    <input>\n",
    "    a: (m, T_x, 2*n_a)\n",
    "    s_prev: (m, n_s)\n",
    "    '''\n",
    "    s_prev = RepeatVector(Tx)(s_prev) # (n_s, ) → (T_x, n_s) / repeat s_prev Tx times\n",
    "    concat = Concatenate(axis=-1)([s_prev,a]) # (T_x, n_s+2*n_a) / concat through last dimension\n",
    "    e = Dense(10, activation=\"tanh\")(concat) # (T_x, 10)\n",
    "    energies = Dense(1, activation=\"relu\")(e) # (T_x, 1)\n",
    "    alphas = Activation(softmax, name='attention_weights')(energies) # (T_x, 1)\n",
    "    context = Dot(axes=1)([alphas,a]) # (1, 2*n_a)\n",
    "    \n",
    "    '''\n",
    "    <output>\n",
    "    context: (m, 2*n_a)\n",
    "    '''\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    '''\n",
    "    m: batch_size(number of training examples in a batch)\n",
    "    X.shape = (m, T_x, human_vocab_size) \n",
    "    Y.shape = (m, T_y, machine_vocab_size))\n",
    "    '''\n",
    "    \n",
    "    X = Input(shape=(Tx, human_vocab_size)) # (T_x, human_vocab_size)  \n",
    "    s0 = Input(shape=(n_s,), name='s0') # initial hidden state\n",
    "    c0 = Input(shape=(n_s,), name='c0') # initial cell state\n",
    "    \n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = [] # (T_y, )\n",
    "\n",
    "    # 1: Pre-attention Bi-LSTM \n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X) # (T_x, 2*n_a)\n",
    "    \n",
    "    # 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "        context = one_step_attention(a, s) # (1, 2*n_a)\n",
    "        _, s, c = LSTM(n_s, return_state = True)(context, initial_state=[s, c]) # (n_s, ), (n_s, )\n",
    "        out = Dense(len(machine_vocab), activation=softmax)(s) # (machine_vocab_size, )\n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = Model(inputs=[X, s0, c0], outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "model.fit([Xoh, s0, c0], list(Yoh.swapaxes(0,1)), epochs=1, batch_size=100)\n",
    "\n",
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "s00 = np.zeros((1, n_s))\n",
    "c00 = np.zeros((1, n_s))\n",
    "for example in EXAMPLES:\n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    #print(source)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    source = np.swapaxes(source, 0, 1)\n",
    "    source = np.expand_dims(source, axis=0)\n",
    "    prediction = model.predict([source, s00, c00])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text generation\n",
    "\n",
    "<img src=\"images/text generation.png\" style=\"width:600;height:400px;\">\n",
    "\n",
    "- need to implement for-loop since we don't know all the values of $x^{\\langle t\\rangle}$ in advance\n",
    "- $\\mathbf{X}$: $(m, T_x, n)$ \n",
    "    - $\\mathbf{x}^{\\langle t \\rangle}$: one-hot vector\n",
    "- $\\mathbf{Y}$: $(T_y, m, n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Reshape\n",
    "\n",
    "\n",
    "n_a = 64\n",
    "n_values = 90\n",
    "reshaper = Reshape((1, n_values))     \n",
    "densor = Dense(n_values, activation='softmax')              \n",
    "LSTM_cell = LSTM(n_a, return_state = True)\n",
    "\n",
    "def train_model(Tx, LSTM_cell, densor, reshaper):\n",
    "   n_values = densor.units\n",
    "   n_a = LSTM_cell.units\n",
    "   \n",
    "   X = Input(shape=(Tx, n_values)) # X: (m, Tx, n_values)\n",
    "   a0 = Input(shape=(n_a,), name='a0')\n",
    "   c0 = Input(shape=(n_a,), name='c0')\n",
    "   a = a0\n",
    "   c = c0\n",
    "   \n",
    "   outputs = [] # outputs: Tx개의 (m, n_values)\n",
    "   \n",
    "   for t in range(Tx):\n",
    "      x = X[:,t,:] # x: (m, n_values)\n",
    "      x = reshaper(x) # x: (m, 1, n_values)\n",
    "      _, a, c = LSTM_cell(x, initial_state=[a, c]) # a: (m, n_a)\n",
    "      out = densor(a) # out: (m, n_values)\n",
    "      outputs.append(out)\n",
    "      \n",
    "   model = Model(inputs=[X, a0, c0], outputs=outputs)\n",
    "   \n",
    "   return model\n",
    "\n",
    "model = train_model(Tx=30, LSTM_cell=LSTM_cell, densor=densor, reshaper=reshaper)\n",
    "\n",
    "m = 60\n",
    "a0 = np.zeros((m, n_a))\n",
    "c0 = np.zeros((m, n_a))\n",
    "history = model.fit([X, a0, c0], list(Y), epochs=100, verbose=0) # list 내의 (m, n_values)별로 loss 계산 후 평균을 내어 gradient descent 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def inference_model(LSTM_cell, densor, Ty):    \n",
    "    n_values = densor.units\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    x0 = Input(shape=(1, n_values))\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    outputs = [] # outputs: (Ty, 1, n_values)\n",
    "    \n",
    "    for t in range(Ty):\n",
    "        _, a, c = LSTM_cell(x, initial_state=[a, c])\n",
    "        out = densor(a) # out: (1, n_values)\n",
    "        outputs.append(out)\n",
    " \n",
    "        '''\n",
    "        argmax: dimension reduction\n",
    "        '''\n",
    "        x = tf.math.argmax(out, axis=-1) # x: (1, )\n",
    "        '''\n",
    "        one_hot: dimension expansion at the end\n",
    "        '''\n",
    "        x = tf.one_hot(x, n_values) # x: (1, n_values)\n",
    "        '''\n",
    "        RepeatVector: dimension expansion after batch_size\n",
    "        '''\n",
    "        x = RepeatVector(1)(x) # x: (1, 1, n_values)\n",
    "        \n",
    "    inference_model = Model(inputs=[x0, a0, c0], outputs=outputs)\n",
    "    \n",
    "    return inference_model\n",
    "\n",
    "model = inference_model(LSTM_cell, densor, Ty=50)\n",
    "\n",
    "x_0 = np.zeros((1, 1, n_values))\n",
    "a_0 = np.zeros((1, n_a))\n",
    "c_0 = np.zeros((1, n_a))\n",
    "\n",
    "def predict_and_sample(inference_model, x_0 = x_0, a_0 = a_0, c_0 = c_0):    \n",
    "    n_values = x_0.shape[2]\n",
    "    \n",
    "    pred = inference_model.predict([x_0, a_0, c_0]) # pred: (Ty, 1, n_values)\n",
    "    indices = np.argmax(np.array(pred), axis=-1) # indices: (Ty, 1)\n",
    "    '''\n",
    "    to_categorical: dimension expansion at the end\n",
    "    '''\n",
    "    results = to_categorical(indices, num_classes=n_values) # results: (Ty, 1, n_values)\n",
    "    \n",
    "    return results, indices\n",
    "\n",
    "results, indices = predict_and_sample(model, x_0, a_0, c_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "character-level text generation\n",
    "\n",
    "<img src=\"images/character-level text generation.png\" style=\"width:500;height:300px;\">\n",
    "\n",
    "- predict the next character\n",
    "- train\n",
    "    - $\\mathbf{X} = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$: a list of character indexes\n",
    "        - $x^{\\langle t \\rangle}$: one-hot vector\n",
    "    - $\\mathbf{Y} = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$: one character index shifted forward\n",
    "        - $y^{\\langle t \\rangle}$: one-hot vector\n",
    "        - $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$\n",
    "- generate through sampling\n",
    "    - $x^{\\langle 1 \\rangle} = \\vec{0}$, $a^{\\langle 0 \\rangle} = \\vec{0}$\n",
    "    - $x^{\\langle t + 1 \\rangle }$: one-hot vector which samples index from the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$\n",
    "        - if most probable character is selected, the model will always generate the same result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient clipping   \n",
    "\n",
    "<img src=\"images/gradient clipping.png\" style=\"width:400;height:150px;\">\n",
    "\n",
    "- solve gradient exploding problem\n",
    "- every element of the gradient vector is clipped to [-N, N]\n",
    "    - if any component of the gradient vector is greater than N, it is set to N\n",
    "    - if any component of the gradient vector is less than -N, it is set to -N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word vectors\n",
    "- word embedding\n",
    "    - use pre-trained set of embeddings\n",
    "- cosine similarity\n",
    "    - measure similarity between 2 words\n",
    "    - $\\text{cosine similarity(u,v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2}$\n",
    "- word analogy\n",
    "    - \"*a* is to *b* as *c* is to **____**\": cosine similarity between $e_b - e_a$ and $e_d - e_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence classification\n",
    "\n",
    "<img src=\"images/sentence classification.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "\n",
    "- padding\n",
    "    - pad all sentences to a maximum length in the training set\n",
    "        - all sentences in the mini-batch should have the same length\n",
    "            - allows vectorization\n",
    "- embedding layer\n",
    "    - embedding layer weights = embedding matrix: (vocab_size, emb_dim)\n",
    "        - embedding matrix rows map word index to embedding vector\n",
    "    - initialize the Embedding layer with pretrained vectors\n",
    "        - if training set is small, leave the embeddings fixed\n",
    "        - if training set is large, update the embeddings\n",
    "    - input: (batch_size, max input length)\n",
    "        - sentences converted into lists of indices\n",
    "    - output: (batch_size, max input length, dimension of word vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Activation\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index): \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    emb_dim = word_to_vec_map[any_word].shape[0] # if GloVe, 50\n",
    "      \n",
    "    emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word] # set each rows with word vectors\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, emb_dim, trainable=False)\n",
    "    embedding_layer.build((None,)) # reset embedding layer weights \n",
    "    embedding_layer.set_weights([emb_matrix]) # set embedding layer weigths to the embedding matrix\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "def sentence_classification_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype='int32') # (m, max_len)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    X = LSTM(128, return_sequences = True)(embeddings)\n",
    "    X = Dropout(0.5)(X) \n",
    "    X = LSTM(128, return_sequences = False)(X)\n",
    "    X = Dropout(0.5)(X) \n",
    "    X = Dense(5)(X)\n",
    "    X = Activation('softmax')(X) # (m, num_class)\n",
    "    \n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model\n",
    "\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"Input_dim\", embedding_layer.input_dim) # embedding layer weights[0] = vocab_size\n",
    "print(\"Output_dim\",embedding_layer.output_dim) # embedding layer weights[1] = emb_dim\n",
    "\n",
    "model = pretrained_embedding_layer((maxLen,), word_to_vec_map, word_to_index)\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = X.shape[0] # X: (m, )                                 \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               \n",
    "        sentence_words = [w.lower() for w in X[i].split()]\n",
    "        j = 0\n",
    "        for w in sentence_words:\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j += 1  \n",
    "    return X_indices\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "# convert input, output to train the model\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)\n",
    "\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)\n",
    "\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print(\"Test accuracy = \", acc)\n",
    "\n",
    "# print mislabelled examplex\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('expected:'+ Y_test[i] + ' prediction: '+ num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speech recognition\n",
    "\n",
    "<img src=\"images/speech recognition.png\" style=\"width:800px;height:700px;\">\n",
    "\n",
    "- CONV-1D\n",
    "\t- extracts features, generate smaller dimension\n",
    "\t\t- speed up model since GRU need to process less timesteps\n",
    "\t- filter size & stride → affect timesteps\n",
    "\t- num_filters → affect the size of vector in each step\n",
    "\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelf(input_shape):\n",
    "    \n",
    "    X_input = Input(shape = input_shape)\n",
    "    \n",
    "    X = Conv1D(filters=196,kernel_size=15,padding='valid',strides=4)(X_input)\n",
    "    \n",
    "    X = BatchNormalization()(X)      \n",
    "    X = Activation('relu')(X) \n",
    "    X = Dropout(0.8)(X)                                      \n",
    "\n",
    "    X = GRU(units = 128, return_sequences = True)(X)\n",
    "    X = Dropout(0.8)(X)\n",
    "    X = BatchNormalization()(X)                            \n",
    "    \n",
    "    X = GRU(units = 128, return_sequences = True)(X)\n",
    "    X = Dropout(0.8)(X)        \n",
    "    X = BatchNormalization()(X)     \n",
    "    X = Dropout(0.8)(X)                                  \n",
    "    \n",
    "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X) # time distributed: apply a layer to each timestep independently \n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ matrix: $(u_{in}, u_{out})$  \n",
    "$b$ vector: ($u_{out},)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiated weights and biases\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights() # model.layers[0].get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights() # model.layers[1].get_weights()\n",
    "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
    "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function: 하나의 input data에 대해 target과 오차를 계산하는 함수  \n",
    "cost function: 모든 input data에 대해 losses를 계산하는 함수로 학습을 통해 이를 최소화 ex) MSE, Binary Cross Entropy, Cross Entropy\n",
    "\n",
    "$$ cost function = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} loss function$$  \n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} L(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ MSE = \\frac{1}{m} \\sum_{i=0}^{m-1} (y^{(i)} - \\hat{y}^{(i)})^2 $$\n",
    "$$ Binary\\ Cross\\ Entropy = -\\frac{1}{m} \\sum_{i=0}^{m-1} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})] $$\n",
    "$$ Cross\\ Entropy = -\\frac{1}{m} \\sum_{i=0}^{m-1} \\sum_{c=1}^{C} y_{c}^{(i)} \\log(\\hat{y}_{c}^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent: $x \\rightarrow x - \\alpha \\nabla f(x)$ 를 통해 cost function을 최소화\n",
    "\n",
    "$$\\begin{align*}& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Gradient Descent\n",
    "- update parameters for entire training dataset\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "m = X.shape[1]  # Number of training examples\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Forward propagation\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    # Compute cost\n",
    "    cost_total = compute_cost(a, Y)  # Cost for m training examples\n",
    "    # Backward propagation\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost\n",
    "    cost_avg = cost_total / m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch Gradient Descent\n",
    "- update parameters for n training examples\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "m = X.shape[1]  # Number of training examples\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    cost_total = 0\n",
    "    for (minibatch_X, minibatch_Y) in minibatches:\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(minibatch_X, parameters)   \n",
    "        # Compute cost\n",
    "        cost_total += compute_cost(a, minibatch_Y)\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost (optional)\n",
    "    cost_avg = cost_total / mini_batch_size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "- update parameters for 1 training example\n",
    "- fast → used when training set is large\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "m = X.shape[1]  # Number of training examples\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    cost_total = 0\n",
    "    for j in range(0, m):\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Compute cost\n",
    "        cost_total += compute_cost(a, Y[:,j])  # Cost for one training example\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost\n",
    "    cost_avg = cost_total / m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate decay: redue learning rate over time to converge to the minimun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics\n",
    "- if labels are heavily skewed, use F1 score/Precision/Recall instead of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "model.compile(\n",
    "    # loss = 'mse' # regression\n",
    "    # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), # binary classification(0/1)\n",
    "    # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # multiclass classification(0~9)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent(efficiency broken into 32 batches)\n",
    "history = model.fit(\n",
    "    Xt,Yt,            \n",
    "    epochs=10,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated weights and biases\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
    "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track the progress of gradient descent\n",
    "def widgvis(fig):\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "\n",
    "def plot_loss_tf(history):\n",
    "    fig,ax = plt.subplots(1,1, figsize = (4,3))\n",
    "    widgvis(fig)\n",
    "    ax.plot(history.history['loss'], label='loss')\n",
    "    ax.set_ylim([0, 2])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('loss (cost)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_tf(history)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "acc = [0.] + history.history['accuracy']\n",
    "val_acc = [0.] + history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "######################################################################\n",
    "\n",
    "df_loss_acc = pd.DataFrame(history.history)\n",
    "df_loss= df_loss_acc[['loss','val_loss']]\n",
    "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
    "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
    "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
    "df_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\n",
    "df_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with loss and metrics\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "X_testn = norm_l(X_test)\n",
    "X_testn = tf.convert_to_tensor(X_testn)\n",
    "\n",
    "first_prediction = model.predict(X_testn[0].reshape(1,400)) \n",
    "print(f\"first prediction: {first_prediction}\")\n",
    "predictions = model.predict(X_testn)\n",
    "print(\"predictions = \\n\", predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification class conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_yhat = (predictions >= 0.5).astype(int) # binary classification(convert probabilites to decision)\n",
    "\n",
    "mc_yhat = tf.nn.softmax(predictions.numpy()) # multiclass classfication(if the desired output is probability)\n",
    "mc_yhat = np.argmax(predictions, axis=1) # multiclass classfication(if the desired output is category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cat_err(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the categorization error\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:|\n",
    "      err: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    incorrect = 0\n",
    "    for i in range(m):\n",
    "        if yhat[i] != y[i]:\n",
    "            incorrect += 1\n",
    "    err = incorrect/m\n",
    "    return(err)\n",
    "\n",
    "training_cerr_complex = eval_cat_err(y_train, model_predict(X_train))\n",
    "cv_cerr_complex = eval_cat_err(y_cv, model_predict(X_cv))\n",
    "print(f\"categorization error, training, complex model: {training_cerr_complex:0.3f}\")\n",
    "print(f\"categorization error, cv,       complex model: {cv_cerr_complex:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = lambda Xl: np.argmax(tf.nn.softmax(model.predict(Xl)).numpy(),axis=1)\n",
    "plt_nn(model_predict,X_train,y_train, classes, X_cv, y_cv, suptitle=\"Complex Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_model(image_shape, data_augmentation=data_augmenter()):\n",
    "    # load the pretrained model weights\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                include_top=True,\n",
    "                                                weights='imagenet') # pretrained weights from ImageNet\n",
    "\n",
    "    # print the summary of the model\n",
    "    print(base_model.summary())\n",
    "\n",
    "    # print the top layers\n",
    "    nb_layers = len(base_model.layers)\n",
    "    print(base_model.layers[nb_layers - 2].name)\n",
    "    print(base_model.layers[nb_layers - 1].name)\n",
    "\n",
    "    # delete the top layers\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                    include_top=False, # delete\n",
    "                                                    weights='imagenet')\n",
    "\n",
    "    # freeze the base model by making it non trainable\n",
    "    base_model.trainable = False \n",
    "\n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = tf.keras.Input(shape=IMG_SHAPE) \n",
    "\n",
    "    # apply data augmentation to the inputs\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # data preprocessing using the same normalization standard\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x) \n",
    "\n",
    "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
    "    x = base_model(x, training=False) \n",
    "\n",
    "    # add the new Binary classification layers\n",
    "    # use global avg pooling to summarize the info in each channel\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x) \n",
    "    # include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "    # use a prediction layer with one neuron (as a binary classifier only needs one)\n",
    "    prediction_layer = tf.keras.layers.Dense(1)\n",
    "    outputs = prediction_layer(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = transfer_model((224,224,3))\n",
    "\n",
    "base_learning_rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "initial_epochs = 5\n",
    "history = model.fit(train_dataset, validation_data=validation_dataset, epochs=initial_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine tuning   \n",
    "- it is especially important to set batchnormalization layers trainable False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model.layers[4]\n",
    "base_model.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 120\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Define a BinaryCrossentropy loss function. Use from_logits=True\n",
    "loss_function=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "# Define an Adam optimizer with a learning rate of 0.1 * base_learning_rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate*0.1)\n",
    "# Use accuracy as evaluation metric\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer = optimizer,\n",
    "              metrics=metrics)\n",
    "\n",
    "fine_tune_epochs = 5\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)\n",
    "\n",
    "acc += history_fine.history['accuracy']\n",
    "val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "loss += history_fine.history['loss']\n",
    "val_loss += history_fine.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "# Set the weights file you downloaded into a variable\n",
    "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# Initialize the base model.\n",
    "# Set the input shape and remove the dense layers.\n",
    "pre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n",
    "                                include_top = False,\n",
    "                                weights = None)\n",
    "\n",
    "# Load the pre-trained weights you downloaded.\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "# Freeze the weights of the layers.\n",
    "for layer in pre_trained_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Choose `mixed7` as the last layer of your base model\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "last_output = last_layer.output\n",
    "print('last layer output: ', last_output)\n",
    "\n",
    "# Print the type of the pre-trained model\n",
    "print(f\"The pretrained model has type: {type(pre_trained_model)}\")\n",
    "\n",
    "# Flatten the output layer to 1 dimension\n",
    "x = layers.Flatten()(last_output)\n",
    "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "# Add a dropout rate of 0.2\n",
    "x = layers.Dropout(0.2)(x)\n",
    "# Add a final sigmoid layer for classification\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Append the dense network to the base model\n",
    "model = Model(\n",
    "    inputs = pre_trained_model.input, \n",
    "    outputs = x\n",
    ")\n",
    "\n",
    "# Print the model summary. See your dense network connected at the end.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = tf.keras.models.load_model('resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pre_trained_model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the training parameters at the end of each epoch  \n",
    "if it meets the threshold, stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    '''\n",
    "    Halts the training when the loss falls below 0.4\n",
    "\n",
    "    Args:\n",
    "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "      logs (dict) - metric results from the training epoch\n",
    "    '''\n",
    "\n",
    "    # Check the loss\n",
    "    if (logs.get('loss') < 0.4):\n",
    "\n",
    "      # Stop if threshold is met\n",
    "      print(\"\\nLoss is lower than 0.4 so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "# Instantiate class\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop if val_loss does not enhance for 2 epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights if val_loss enhances(Early Stopping으로 훈련이 중단된 시점이 아니라, 검증 성능이 가장 좋았던 시점의 모델 가중치를 사용 가능)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.h5',  # 저장할 파일 이름\n",
    "    save_best_only=True,  # 최적의 모델만 저장\n",
    "    monitor='val_loss',  # 모니터링할 지표\n",
    "    mode='min',  # 'min'은 val_loss를 모니터링할 때 사용\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    callbacks=[callbacks, early_stopping, model_checkpoint], \n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speed up gradient descent  \n",
    "avoid vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bias/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the baseline level of performance(human performance)  \n",
    "therefore, consider the gap between training error and cross validation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{just right: } J_{\\text{train}} \\downarrow, J_{\\text{cv}} \\downarrow$  \n",
    "\n",
    "$\\text{high bias problem/underfitting: } J_{\\text{train}} \\uparrow, J_{\\text{cv}} \\uparrow/{\\text{ training loss is far wrose than the baseline level of performance}}$\n",
    "* try adding polynomial features\n",
    "* try getting additional features\n",
    "* try decreasing the regularization parameter  \n",
    "\n",
    "$\\text{high variance problem/overfitting: } J_{\\text{train}} \\downarrow, J_{\\text{cv}} \\uparrow$\n",
    "* try increasing the regularization parameter\n",
    "* try smaller sets of features\n",
    "* get more training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 regularization\n",
    "* generalize to not learn complex patterns in data\n",
    "* encourages gradient descent to minimize $\\mathbf{w}$\n",
    "* used only in training\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{1}{2}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{linear regression} $$ \n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{logistic regression} $$\n",
    "$$\\begin{align*} \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\\\ \\end{align*}$$\n",
    "$$\\begin{align*}&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}   \\; & \\text{for j := 0..n-1} \\\\ \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r = Sequential(\n",
    "    [\n",
    "        Dense(units=120, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "        Dense(units=40, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "        Dense(units=6, activation='linear'),\n",
    "    ], name= None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout\n",
    "* in each iteration randomly shut down some neruons for both forward and backward propatations\n",
    "* able to not rely on specific neurons\n",
    "* used only in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models():\n",
    "    \n",
    "    tf.random.set_seed(20)\n",
    "    \n",
    "    model_1 = Sequential(\n",
    "        [\n",
    "            Dense(25, activation = 'relu'),\n",
    "            Dense(15, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_1'\n",
    "    )\n",
    "\n",
    "    model_2 = Sequential(\n",
    "        [\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_2'\n",
    "    )\n",
    "\n",
    "    model_3 = Sequential(\n",
    "        [\n",
    "            Dense(32, activation = 'relu'),\n",
    "            Dense(16, activation = 'relu'),\n",
    "            Dense(8, activation = 'relu'),\n",
    "            Dense(4, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_3'\n",
    "    )\n",
    "    \n",
    "    model_list = [model_1, model_2, model_3]\n",
    "    \n",
    "    return model_list\n",
    "\n",
    "\n",
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_mses = []\n",
    "nn_cv_mses = []\n",
    "\n",
    "# Build the models\n",
    "nn_models = build_models()\n",
    "\n",
    "# Loop over the the models\n",
    "for model in nn_models:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_mapped_scaled, y_train,\n",
    "        epochs=300,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    \n",
    "    # Record the training MSEs\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    nn_train_mses.append(train_mse)\n",
    "    \n",
    "    # Record the cross validation MSEs \n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    nn_cv_mses.append(cv_mse)\n",
    "\n",
    "    \n",
    "# print results\n",
    "print(\"RESULTS:\")\n",
    "for model_num in range(len(nn_train_mses)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
    "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
    "        )\n",
    "    \n",
    "# Select the model with the lowest CV MSE\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification models(error metric: fraction of the data that the model has misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_error = []\n",
    "nn_cv_error = []\n",
    "\n",
    "# Build the models\n",
    "models_bc = utils.build_models()\n",
    "\n",
    "# Loop over each model\n",
    "for model in models_bc:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_bc_train_scaled, y_bc_train,\n",
    "        epochs=200,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "    \n",
    "    # Set the threshold for classification\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Record the fraction of misclassified examples for the training set\n",
    "    yhat = model.predict(x_bc_train_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    train_error = np.mean(yhat != y_bc_train)\n",
    "    nn_train_error.append(train_error)\n",
    "\n",
    "    # Record the fraction of misclassified examples for the cross validation set\n",
    "    yhat = model.predict(x_bc_cv_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    cv_error = np.mean(yhat != y_bc_cv)\n",
    "    nn_cv_error.append(cv_error)\n",
    "\n",
    "# Print the result\n",
    "for model_num in range(len(nn_train_error)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
    "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest error\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test error\n",
    "yhat = models_bc[model_num-1].predict(x_bc_test_scaled)\n",
    "yhat = tf.math.sigmoid(yhat)\n",
    "yhat = np.where(yhat >= threshold, 1, 0)\n",
    "nn_test_error = np.mean(yhat != y_bc_test)\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\n",
    "print(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\n",
    "print(f\"Test Set Classification Error: {nn_test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for reg_param in reg_params:\n",
    "\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model = Ridge(alpha=reg_param)\n",
    "        model.fit(X_train_mapped_scaled, y_train)\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "        cv_mses.append(cv_mse)\n",
    "\n",
    "    # Plot the results\n",
    "    reg_params = [str(x) for x in reg_params]\n",
    "    plt.plot(reg_params, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(reg_params, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(reg_params, np.repeat(baseline, len(reg_params)), linestyle='--', label='baseline')\n",
    "    plt.title(\"lambda vs. train and CV MSEs\")\n",
    "    plt.xlabel(\"lambda\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define lambdas to plot\n",
    "reg_params = [10, 5, 2, 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01]\n",
    "\n",
    "# Define degree of polynomial and train for each value of lambda\n",
    "train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_range = np.array([0.0, 1e-6, 1e-5, 1e-4,1e-3,1e-2, 1e-1,1,10,100])\n",
    "num_steps = len(lambda_range)\n",
    "degree = 10\n",
    "err_train = np.zeros(num_steps)    \n",
    "err_cv = np.zeros(num_steps)       \n",
    "x = np.linspace(0,int(X.max()),100) \n",
    "y_pred = np.zeros((100,num_steps))  #columns are lines to plot\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lambda_= lambda_range[i]\n",
    "    lmodel = lin_model(degree, regularization=True, lambda_=lambda_)\n",
    "    lmodel.fit(X_train, y_train)\n",
    "    yhat = lmodel.predict(X_train)\n",
    "    err_train[i] = lmodel.mse(y_train, yhat)\n",
    "    yhat = lmodel.predict(X_cv)\n",
    "    err_cv[i] = lmodel.mse(y_cv, yhat)\n",
    "    y_pred[:,i] = lmodel.predict(x)\n",
    "    \n",
    "optimal_reg_idx = np.argmin(err_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "models=[None] * len(lambdas)\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    lambda_ = lambdas[i]\n",
    "    models[i] =  Sequential(\n",
    "        [\n",
    "            Dense(120, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "            Dense(40, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "            Dense(classes, activation = 'linear')\n",
    "        ]\n",
    "    )\n",
    "    models[i].compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "    )\n",
    "\n",
    "    models[i].fit(\n",
    "        X_train,y_train,\n",
    "        epochs=1000\n",
    "    )\n",
    "    print(f\"Finished lambda = {lambda_}\")\n",
    "\n",
    "def plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv):\n",
    "    err_train = np.zeros(len(lambdas))\n",
    "    err_cv = np.zeros(len(lambdas))\n",
    "    for i in range(len(models)):\n",
    "        err_train[i] = eval_cat_err(y_train,np.argmax( models[i](X_train), axis=1))\n",
    "        err_cv[i] = eval_cat_err(y_cv, np.argmax( models[i](X_cv), axis=1))\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "    ax.set_title(\"error vs regularization\",fontsize = 12)\n",
    "    ax.plot(lambdas, err_train, marker='o', label=\"train error\", color = dlc[\"dlblue\"])\n",
    "    ax.plot(lambdas, err_cv,    marker='o', label=\"cv error\",    color = dlc[\"dlorange\"])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(*ax.get_ylim())\n",
    "    ax.set_xlabel(\"Regularization (lambda)\",fontsize = 14)\n",
    "    ax.set_ylabel(\"Error\",fontsize = 14)\n",
    "    ax.legend()\n",
    "    fig.suptitle(\"Tuning Regularization\",fontsize = 14)\n",
    "    ax.text(0.05,0.14,\"Training Error\\nlower than CV\",fontsize=12, ha='left',transform=ax.transAxes,color = dlc[\"dlblue\"])\n",
    "    ax.text(0.95,0.14,\"Similar\\nTraining, CV\",    fontsize=12, ha='right',transform=ax.transAxes,color = dlc[\"dlblue\"])\n",
    "    plt.show()\n",
    "\n",
    "plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neural network(unsupervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train from normal examples and filter examples that aren't normal \n",
    "* anomaly detection vs supervised learning 1\n",
    "    1. anomaly detection: capable of finding previously unseen defects\n",
    "    2. supervised learning: capable of finding previously seen defects\n",
    "* anomaly detection vs supervised learning 2\n",
    "    1. anomaly detection: needs to choose what features to use\n",
    "    2. supervised learning: don't need to choose what features to use (able to learn which features to not use from labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. draw histogram for each features\n",
    "    $$p(x_i ; \\mu_i,\\sigma_i ^2) \\rightarrow \\mathbf{plt.hist(x)}$$\n",
    "2. make non-gaussian features more like gaussian (don't forget to apply to cross validation, test set)\n",
    "    $$log(x_i)$$\n",
    "    $$log(x_i + C)$$\n",
    "    $$\\sqrt[2]{x_i}$$\n",
    "    $$\\sqrt[3]{x_i}$$\n",
    "2. estimate Gaussian distribution from features ($x_i$ &rightarrow; $\\mu_i$, $\\sigma_i^2$)\n",
    "$$\\mu_i = \\frac{1}{m} \\sum_{j=1}^m x_i^{(j)}$$\n",
    "$$\\sigma_i^2 = \\frac{1}{m} \\sum_{j=1}^m (x_i^{(j)} - \\mu_i)^2$$\n",
    "$$p(x_i ; \\mu_i,\\sigma_i ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^2}}\\exp^{ - \\frac{(x_i - \\mu_i)^2}{2 \\sigma_i ^2} }$$\n",
    "3. identify anomalous example\n",
    "    * want $p(x)$ large for normal examples and small for anomalous examples\n",
    "    * if $p(x) is comparable(large) for normal and anomalous examples, add more features to achieve above\n",
    "$$p(x) = \\prod_{i=1}^{n} p(x_i; \\mu_i, \\sigma_i^2)$$\n",
    "$$if\\ anomaly,\\ p(x) < \\epsilon\\rightarrow y = 1$$\n",
    "4. select best $\\epsilon$ using F1 score\n",
    "$$F_1 = \\frac{2\\cdot prec \\cdot rec}{prec + rec}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate mean and variance of each feature\n",
    "def estimate_gaussian(X): \n",
    "    \"\"\"\n",
    "    Calculates mean and variance of all features \n",
    "    in the dataset\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Data matrix\n",
    "    \n",
    "    Returns:\n",
    "        mu (ndarray): (n,) Mean of all features\n",
    "        var (ndarray): (n,) Variance of all features\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    mu = 1/m * np.sum(X, axis=0)\n",
    "    var = 1/m * np.sum((X-mu)**2, axis=0)\n",
    "        \n",
    "    return mu, var\n",
    "\n",
    "mu, var = estimate_gaussian(X_train)              \n",
    "\n",
    "print(\"Mean of each feature:\", mu)\n",
    "print(\"Variance of each feature:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the probabilites for the training set\n",
    "def multivariate_gaussian(X, mu, var):\n",
    "    \"\"\"\n",
    "    Computes the probability \n",
    "    density function of the examples X under the multivariate gaussian \n",
    "    distribution with parameters mu and var. If var is a matrix, it is\n",
    "    treated as the covariance matrix. If var is a vector, it is treated\n",
    "    as the var values of the variances in each dimension (a diagonal\n",
    "    covariance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    k = len(mu)\n",
    "    \n",
    "    if var.ndim == 1:\n",
    "        var = np.diag(var)\n",
    "        \n",
    "    X = X - mu\n",
    "    p = (2* np.pi)**(-k/2) * np.linalg.det(var)**(-0.5) * \\\n",
    "        np.exp(-0.5 * np.sum(np.matmul(X, np.linalg.pinv(var)) * X, axis=1))\n",
    "    \n",
    "    return p\n",
    "\n",
    "p_train = multivariate_gaussian(X_train, mu, var)\n",
    "p_val = multivariate_gaussian(X_val, mu, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best threshold\n",
    "def select_threshold(y_val, p_val): \n",
    "    \"\"\"\n",
    "    Finds the best threshold to use for selecting outliers \n",
    "    based on the results from a validation set (p_val) \n",
    "    and the ground truth (y_val)\n",
    "    \n",
    "    Args:\n",
    "        y_val (ndarray): Ground truth on validation set\n",
    "        p_val (ndarray): Results on validation set\n",
    "        \n",
    "    Returns:\n",
    "        epsilon (float): Threshold chosen \n",
    "        F1 (float):      F1 score by choosing epsilon as threshold\n",
    "    \"\"\" \n",
    "\n",
    "    best_epsilon = 0\n",
    "    best_F1 = 0\n",
    "    F1 = 0\n",
    "    \n",
    "    step_size = (max(p_val) - min(p_val)) / 1000\n",
    "    \n",
    "    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n",
    "        \n",
    "        predictions = p_val < epsilon\n",
    "\n",
    "        tp = np.sum((predictions == 1) & (y_val == 1))\n",
    "        fp = np.sum((predictions == 1) & (y_val == 0))\n",
    "        fn = np.sum((predictions == 0) & (y_val == 1))\n",
    "\n",
    "        prec = tp / (tp + fp)\n",
    "        rec = tp / (tp + fn)\n",
    "\n",
    "        F1 = 2 * prec * rec / (prec + rec)\n",
    "        \n",
    "        if F1 > best_F1:\n",
    "            best_F1 = F1\n",
    "            best_epsilon = epsilon\n",
    "        \n",
    "    return best_epsilon, best_F1\n",
    "\n",
    "epsilon, F1 = select_threshold(y_val, p_val)\n",
    "\n",
    "print('Best epsilon found using cross-validation: %e' % epsilon)\n",
    "print('Best F1 on Cross Validation Set: %f' % F1)\n",
    "print('# Anomalies found: %d'% sum(p_val < epsilon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
