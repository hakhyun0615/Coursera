{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4) :   a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.zeros(4,) :  a = [0. 0. 0. 0.], a shape = (4,), a data type = float64\n",
      "np.random.random_sample(4): a = [0.22657227 0.54141137 0.53050087 0.7931722 ], a shape = (4,), a data type = float64\n",
      "np.arange(4.):     a = [0. 1. 2. 3.], a shape = (4,), a data type = float64\n",
      "np.random.rand(4): a = [0.82546165 0.49381691 0.87118799 0.4452837 ], a shape = (4,), a data type = float64\n",
      "np.array([5,4,3,2]):  a = [5 4 3 2],     a shape = (4,), a data type = int64\n",
      "np.array([5.,4,3,2]): a = [5. 4. 3. 2.], a shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(4);                print(f\"np.zeros(4) :   a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.zeros((4,));             print(f\"np.zeros(4,) :  a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.random_sample(4); print(f\"np.random.random_sample(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.arange(4.);              print(f\"np.arange(4.):     a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.rand(4);          print(f\"np.random.rand(4): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5,4,3,2]);  print(f\"np.array([5,4,3,2]):  a = {a},     a shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5.,4,3,2]); print(f\"np.array([5.,4,3,2]): a = {a}, a shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a             : [1 2 3 4]\n",
      "b = a+3       : [4 5 6 7]\n",
      "b = -a        : [-1 -2 -3 -4]\n",
      "b = np.sum(a) : 10\n",
      "b = np.mean(a): 2.5\n",
      "b = np.exp(a) : [ 2.71828183  7.3890561  20.08553692 54.59815003]\n",
      "b = a**2      : [ 1  4  9 16]\n",
      "b = sigmoid(a): [0.73105858 0.88079708 0.95257413 0.98201379]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "print(f\"a             : {a}\")\n",
    "\n",
    "b = a+3 \n",
    "print(f\"b = a+3       : {b}\")\n",
    "b = -a \n",
    "print(f\"b = -a        : {b}\")\n",
    "b = np.sum(a) \n",
    "print(f\"b = np.sum(a) : {b}\")\n",
    "b = np.mean(a)\n",
    "print(f\"b = np.mean(a): {b}\")\n",
    "b = np.exp(a)\n",
    "print(f\"b = np.exp(a) : {b}\")\n",
    "b = a**2\n",
    "print(f\"b = a**2      : {b}\")\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1+np.exp(-x))\n",
    "    return s\n",
    "b = sigmoid(a)\n",
    "print(f\"b = sigmoid(a): {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([ 1, 2, 3, 4])\n",
    "b = np.array([-1,-2, 3, 4])\n",
    "print(f\"Binary operators work element wise: {a + b}\")\n",
    "\n",
    "c = np.array([1, 2])\n",
    "try:\n",
    "    d = a + c\n",
    "except Exception as e:\n",
    "    print(\"The error message you'll see is:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dot/inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a \\cdot b = a^T b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1-D np.dot(a, b) = 24, np.dot(a, b).shape = () \n",
      "NumPy 1-D np.dot(b, a) = 24, np.dot(a, b).shape = () \n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "c = np.dot(a, b)\n",
    "print(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \") \n",
    "c = np.dot(b, a)\n",
    "print(f\"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((1, 5))                                       \n",
    "print(f\"a shape = {a.shape}, a = {a}\")                     \n",
    "a = np.zeros((2, 1))                                                                   \n",
    "print(f\"a shape = {a.shape}, a = {a}\") \n",
    "a = np.random.random_sample((1, 1))  \n",
    "print(f\"a shape = {a.shape}, a = {a}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6).reshape(-1, 2) \n",
    "print(f\"a.shape: {a.shape}, \\na= {a}\")\n",
    "print(f\"a[2,0].shape:   {a[2, 0].shape}, a[2,0] = {a[2, 0]},     type(a[2,0]) = {type(a[2, 0])} Accessing an element returns a scalar\")\n",
    "print(f\"a[2].shape:   {a[2].shape}, a[2]   = {a[2]}, type(a[2])   = {type(a[2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b shape: (1, 3), \n",
      "a + b = [[6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3]).reshape(1,-1)  # (1,3)\n",
    "b = 5\n",
    "print(f\"a + b shape: {(a + b).shape}, \\na + b = {a + b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b shape: (3, 4), \n",
      "a + b = \n",
      "[[2 3 4 5]\n",
      " [3 4 5 6]\n",
      " [4 5 6 7]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4]).reshape(1,-1) # (1,4)\n",
    "b = np.array([1,2,3]).reshape(-1,1) # (3,1)\n",
    "print(f\"a + b shape: {(a + b).shape}, \\na + b = \\n{a + b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2)\n",
      "(18, 1)\n"
     ]
    }
   ],
   "source": [
    "# image2vector\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                     [ 0.90714982,  0.52835647],\n",
    "                     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                   [[ 0.92814219,  0.96677647],\n",
    "                    [ 0.85304703,  0.52351845],\n",
    "                    [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                   [[ 0.60659855,  0.00533165],\n",
    "                    [ 0.10820313,  0.49978937],\n",
    "                    [ 0.34144279,  0.94630077]]]) # (length, height, 3)\n",
    "vector = image.reshape(-1,1) # (length*height*3, 1)\n",
    "print(image.shape)\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### normalize rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = \\begin{bmatrix}\n",
    "        0 & 3 & 4 \\\\\n",
    "        2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}$$ \n",
    "$$\\| x\\| = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 4]\n",
      " [2 6 4]]\n",
      "[[5.        ]\n",
      " [7.48331477]]\n",
      "[[0.         0.6        0.8       ]\n",
      " [0.26726124 0.80178373 0.53452248]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 3, 4],\n",
    "              [2, 6, 4]])\n",
    "print(x)\n",
    "x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True) # axis=1: row-wise\n",
    "print(x_norm)\n",
    "x = x / x_norm\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}$$\n",
    "$$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n",
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "def L1(yhat, y):\n",
    "    loss = np.sum(np.abs(y-yhat))\n",
    "    return loss\n",
    "def L2(yhat, y):\n",
    "    loss = np.sum(np.square(y-yhat))\n",
    "    return loss\n",
    "\n",
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))\n",
    "print(\"L2 = \" + str(L2(yhat, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### abstraction of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = \n",
    "\\left(\\begin{array}{cc} \n",
    "--- x^{(1)} --- \\\\\n",
    "--- x^{(2)} --- \\\\\n",
    "\\vdots \\\\ \n",
    "--- x^{(m)} --- \n",
    "\\end{array}\\right)\n",
    "Y = \n",
    "\\left(\\begin{array}{cc} \n",
    " y^{(1)}  \\\\\n",
    " y^{(2)}  \\\\\n",
    "\\vdots \\\\ \n",
    " y^{(m)}  \n",
    "\\end{array}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type of x_train:\", type(x_train))\n",
    "print ('The first element of X is: ', x_train[0])\n",
    "print(\"First five elements of x_train are:\\n\", x_train[:5]) \n",
    "\n",
    "print(\"Type of y_train:\", type(y_train))\n",
    "print ('The first element of y is: ', y_train[0,0])\n",
    "print(\"First five elements of y_train are:\\n\", y_train[:5])  \n",
    "\n",
    "print ('The shape of x_train is:', x_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show classes in data set\n",
    "print(f\"unique classes {np.unique(y_train)}\")\n",
    "# show how classes are represented\n",
    "print(f\"class representation {y_train[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scatter plot: indictation of which features have the strongest influence on target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.scatter(X_train, Y_train, marker='x', c='r', label=\"Data Points\")\n",
    "ax.legend( fontsize='xx-large')\n",
    "ax.set_ylabel('Price (in 1000s of dollars)', fontsize='xx-large')\n",
    "ax.set_xlabel('Size (1000 sqft)', fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1, 4, figsize=(12, 3), sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i], y_train)\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "ax[0].set_ylabel(\"Price (1000's)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(x, y, title):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "    plt.rcParams[\"lines.markersize\"] = 12\n",
    "    plt.scatter(x, y, marker='x', c='r')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\") \n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(x=x, y=y, title=\"input vs. target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = Y_train == 1\n",
    "neg = Y_train == 0\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
    "ax.scatter(X_train[pos], Y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
    "ax.scatter(X_train[neg], Y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', \n",
    "              edgecolors=dlc[\"dlblue\"],lw=3)\n",
    "\n",
    "ax.set_ylim(-0.08,1.1)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_title('one variable plot')\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bc_dataset(x, y, title):\n",
    "    for i in range(len(y)):\n",
    "        marker = 'x' if y[i] == 1 else 'o'\n",
    "        c = 'r' if y[i] == 1 else 'b'\n",
    "        plt.scatter(x[i,0], x[i,1], marker=marker, c=c); \n",
    "    plt.title(\"x1 vs x2\")\n",
    "    plt.xlabel(\"x1\"); \n",
    "    plt.ylabel(\"x2\"); \n",
    "    y_0 = mlines.Line2D([], [], color='r', marker='x', markersize=12, linestyle='None', label='y=1')\n",
    "    y_1 = mlines.Line2D([], [], color='b', marker='o', markersize=12, linestyle='None', label='y=0')\n",
    "    plt.title(title)\n",
    "    plt.legend(handles=[y_0, y_1])\n",
    "    plt.show()\n",
    "\n",
    "plot_bc_dataset(x=x_bc, y=y_bc, title=\"x1 vs. x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature engineering (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using domain knowledge to design new features by transforming a feature or combining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add polynomial features    \n",
    "\n",
    "<img src='organize_picture_1.png' width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "X_cv_mapped = poly.transform(x_cv)\n",
    "X_test_mapped = poly.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_cv_mses(degrees, train_mses, cv_mses, title):\n",
    "    degrees = range(1,11)\n",
    "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Initialize lists to save the errors, models, and feature transforms\n",
    "train_mses = []\n",
    "cv_mses = []\n",
    "models = []\n",
    "polys = []\n",
    "scalers = []\n",
    "\n",
    "# Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "for degree in range(1,11):\n",
    "    \n",
    "    # Add polynomial features to the training set\n",
    "    poly = PolynomialFeatures(degree, include_bias=False)\n",
    "    X_train_mapped = poly.fit_transform(x_train)\n",
    "    polys.append(poly)\n",
    "    \n",
    "    # Scale the training set\n",
    "    scaler_poly = StandardScaler()\n",
    "    X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "    scalers.append(scaler_poly)\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_mapped_scaled, y_train )\n",
    "    models.append(model)\n",
    "    \n",
    "    # Compute the training MSE\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    train_mses.append(train_mse)\n",
    "    \n",
    "    # Add polynomial features and scale the cross validation set\n",
    "    X_cv_mapped = poly.transform(x_cv)\n",
    "    X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "    \n",
    "    # Compute the cross validation MSE\n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    cv_mses.append(cv_mse)\n",
    "    \n",
    "# Plot the results\n",
    "degrees=range(1,11)\n",
    "plot_train_cv_mses(degrees, train_mses, cv_mses, title=\"degree of polynomial vs. train and CV MSEs\")\n",
    "\n",
    "# Get the model with the lowest CV MSE (add 1 because list indices start at 0)\n",
    "# This also corresponds to the degree of the polynomial added\n",
    "degree = np.argmin(cv_mses) + 1\n",
    "print(f\"Lowest CV MSE is found in the model with degree={degree}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "    degrees = range(1,max_degree+1)\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for degree in degrees:\n",
    "\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model.fit(X_train_mapped_scaled, y_train )\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "        cv_mses.append(cv_mse)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.plot(degrees, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(degrees, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(degrees, np.repeat(baseline, len(degrees)), linestyle='--', label='baseline')\n",
    "    plt.title(\"degree of polynomial vs. train and CV MSEs\")\n",
    "    plt.xticks(degrees)\n",
    "    plt.xlabel(\"degree\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train_plot_poly(model, x_train, y_train, x_cv, y_cv, max_degree=10, baseline=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorical variable with n outputs &rightarrow; n binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data = df,\n",
    "                    prefix = cat_variables,\n",
    "                    columns = cat_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training set: used to train the model  \n",
    "cross validation set: used to tune different model configurations and differentiate between underfitting and overfitting\n",
    "test set: used to represent new data and estimate chosen model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# training set: 60, cross validation set: 20, test set: 20\n",
    "x_train, x_, y_train, y_ = train_test_split(x, y, test_size=0.40, random_state=1)\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title):\n",
    "    plt.scatter(x_train, y_train, marker='x', c='r', label='training'); \n",
    "    plt.scatter(x_cv, y_cv, marker='o', c='b', label='cross validation'); \n",
    "    plt.scatter(x_test, y_test, marker='^', c='g', label='test'); \n",
    "    plt.title(\"input vs. target\")\n",
    "    plt.xlabel(\"x\"); \n",
    "    plt.ylabel(\"y\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_train_cv_test(x_train, y_train, x_cv, y_cv, x_test, y_test, title=\"input vs. target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "ax.set_title(\"Training, CV, Test\",fontsize = 14)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "\n",
    "ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "ax.scatter(X_cv, y_cv,       color = dlc[\"dlorange\"], label=\"cv\")\n",
    "ax.scatter(X_test, y_test,   color = dlc[\"dlblue\"],   label=\"test\")\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. some labeled anomalous examples\n",
    "    * training set: 0.6 * normal\n",
    "    * cross validation set: 0.2 * normal + 0.5 * anomalous &rightarrow; tune $x_i,\\epsilon$\n",
    "    * test set: 0.2 * normal + 0.5 * anomalous\n",
    "2. few labeled anomalous examples (higher risk of overfitting)\n",
    "    * training set: 0.6 * normal\n",
    "    * cross validation set: 0.4 * normal + anomalous &rightarrow; tune $x_i,\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make feature have similar range(-1~1) to speed up gradient descent\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "$$z \\sim N(0,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_mapped_scaled = scaler.fit_transform(X_train_mapped)\n",
    "print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train_mapped_scaled,axis=0)}\")   \n",
    "print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_train_mapped_scaled,axis=0)}\")\n",
    "\n",
    "X_cv_mapped_scaled = scaler.transform(X_cv_mapped)\n",
    "X_test_mapped_scaled = scalers.transform(X_test_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(x, y, title):\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "    plt.rcParams[\"lines.markersize\"] = 12\n",
    "    plt.scatter(x, y, marker='x', c='r')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x\") \n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "plot_dataset(x=X_norm, y=y_train, title=\"scaled input vs. target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"Temperature Max, Min pre normalization: {np.max(X[:,0]):0.2f}, {np.min(X[:,0]):0.2f}\")\n",
    "print(f\"Duration    Max, Min pre normalization: {np.max(X[:,1]):0.2f}, {np.min(X[:,1]):0.2f}\")\n",
    "norm_l = tf.keras.layers.Normalization(axis=-1)\n",
    "norm_l.adapt(X)  # learns mean, variance\n",
    "Xn = norm_l(X)\n",
    "print(f\"Temperature Max, Min post normalization: {np.max(Xn[:,0]):0.2f}, {np.min(Xn[:,0]):0.2f}\")\n",
    "print(f\"Duration    Max, Min post normalization: {np.max(Xn[:,1]):0.2f}, {np.min(Xn[:,1]):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "measures the cost of the cross validation set to early stop, automatically control estimators(number of rounds) if the cost on the cross validation set stops to decrease for a number of early_stopping_rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators = 500, learning_rate = 0.1,verbosity = 1, random_state = RANDOM_STATE)\n",
    "xgb_model.fit(X_train_fit,y_train_fit, eval_set = [(X_train_eval,y_train_eval)], early_stopping_rounds = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"Number of fitted trees which achieved the minimum value of the log-loss: {xgb_model.best_iteration}\")\n",
    "print(f\"Metrics train:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_train),y_train):.4f}\\nMetrics test:\\n\\tAccuracy score: {accuracy_score(xgb_model.predict(X_test),y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f_{\\mathbf{w},b}(x^{(i)}) = \\mathbf{w}\\cdot x^{(i)} + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to minimize a cost function in order to get optimal parameters\n",
    "1. Normal Equation: $w^* = (X^T X)^{-1} X^T Y$\n",
    "2. Gradient Descent: $w := w - \\alpha (X^T (Xw - Y))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Closed-Form Solution\n",
    "- 데이터 전체를 한 번에 사용하여 mathmatically Normal Equation을 통해 가중치를 업데이트\n",
    "- 작은 데이터셋에서 사용\n",
    "- Normalization 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train) # X: 2d Matrix, y: 1d vector\n",
    "\n",
    "b = linear_model.intercept_\n",
    "w = linear_model.coef_\n",
    "print(f\"w = {w:}, b = {b:0.2f}\")\n",
    "\n",
    "yhat = linear_model.predict(X_train)\n",
    "print(f\"Prediction on training set:\\n {yhat}\" )\n",
    "print(f\"Target values \\n {y_train}\")\n",
    "print(f\"Training MSE: {mean_squared_error(y_train, yhat) / 2}\")\n",
    "\n",
    "yhat = linear_model.predict(X_cv_norm)\n",
    "print(f\"Cross validation MSE: {mean_squared_error(y_cv, yhat) / 2}\")\n",
    "\n",
    "yhat = models[degree-1].predict(X_test_mapped_scaled)\n",
    "print(f\"Test MSE: {mean_squared_error(y_test, yhat) / 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stochastic Gradient Descent\n",
    "- 한 번에 하나의 샘플을 사용하여 Gradient Descent를 사용하여 가중치를 업데이트\n",
    "- 큰 데이터셋에서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgdr = SGDRegressor(max_iter=1000)\n",
    "sgdr.fit(X_norm, y_train) # X: 2d Matrix, y: 1d vector\n",
    "print(sgdr)\n",
    "print(f\"number of iterations completed: {sgdr.n_iter_}, number of weight updates: {sgdr.t_}\")\n",
    "print(f\"model parameters: w: {sgdr.intercept_}, b:{sgdr.coef_}\")\n",
    "\n",
    "y_pred_sgd = sgdr.predict(X_norm)\n",
    "print(f\"prediction using sgdr.predict: {y_pred_sgd}\")\n",
    "\n",
    "print(f\"Prediction on training set:\\n{y_pred_sgd[:4]}\" )\n",
    "print(f\"Target values \\n{y_train[:4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions and targets vs original features    \n",
    "fig,ax=plt.subplots(1,4,figsize=(12,3),sharey=True)\n",
    "for i in range(len(ax)):\n",
    "    ax[i].scatter(X_train[:,i],y_train, label = 'target')\n",
    "    ax[i].set_xlabel(X_features[i])\n",
    "    ax[i].scatter(X_train[:,i],y_pred,color=dlc[\"dlorange\"], label = 'predict')\n",
    "ax[0].set_ylabel(\"Price\"); ax[0].legend()\n",
    "fig.suptitle(\"target versus prediction using z-score normalized model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions over data range \n",
    "\n",
    "def plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "\n",
    "    ax.set_title(\"Poor Performance on Test Data\",fontsize = 12)\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "\n",
    "    ax.scatter(X_train, y_train, color = \"red\",           label=\"train\")\n",
    "    ax.scatter(X_test, y_test,       color = dlc[\"dlblue\"], label=\"test\")\n",
    "    ax.set_xlim(ax.get_xlim())\n",
    "    ax.set_ylim(ax.get_ylim())\n",
    "    ax.plot(x, y_pred,  lw=0.5, label=f\"predicted, degree={degree}\")\n",
    "    ax.plot(x_ideal, y_ideal, \"--\", color = \"orangered\", label=\"y_ideal\", lw=1)\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "x = np.linspace(0,int(X.max()),100)  # predict values for plot\n",
    "y_pred = lmodel.predict(x).reshape(-1,1)\n",
    "\n",
    "plt_train_test(X_train, y_train, X_test, y_test, x, y_pred, x_ideal, y_ideal, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "- one neuron\n",
    "- no activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "linear_layer = tf.keras.layers.Dense(units=1, activation='linear')\n",
    "a1 = linear_layer(X_train[0].reshape(1,1))\n",
    "print(a1)\n",
    "\n",
    "w, b= linear_layer.get_weights()\n",
    "print(f\"w = {w}, b={b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression: linear regression + sigmoid function\n",
    "\n",
    "$$ 0 \\leq f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\leq 1 $$\n",
    "$$ \\text{if } \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b \\geq 0 \\text{ or } f_{\\mathbf{w},b}(x) \\geq 0.5, \\text{ predict } y=1 $$\n",
    "$$ \\text{if } \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b < 0 \\text{ or } f_{\\mathbf{w},b}(x) < 0.5, \\text{ predict } y=0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "y_pred = lr_model.predict(X)\n",
    "print(\"Prediction on training set:\", y_pred)\n",
    "\n",
    "print(\"Accuracy on training set:\", lr_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(w, b, X, y):\n",
    "    # Credit to dibgerge on Github for this plotting code\n",
    "     \n",
    "    plot_data(X[:, 0:2], y)\n",
    "    \n",
    "    if X.shape[1] <= 2:\n",
    "        plot_x = np.array([min(X[:, 0]), max(X[:, 0])])\n",
    "        plot_y = (-1. / w[1]) * (w[0] * plot_x + b)\n",
    "        \n",
    "        plt.plot(plot_x, plot_y, c=\"b\")\n",
    "        \n",
    "    else:\n",
    "        u = np.linspace(-1, 1.5, 50)\n",
    "        v = np.linspace(-1, 1.5, 50)\n",
    "        \n",
    "        z = np.zeros((len(u), len(v)))\n",
    "\n",
    "        # Evaluate z = theta*x over the grid\n",
    "        for i in range(len(u)):\n",
    "            for j in range(len(v)):\n",
    "                z[i,j] = sig(np.dot(map_feature(u[i], v[j]), w) + b)\n",
    "        \n",
    "        # important to transpose z before calling contour       \n",
    "        z = z.T\n",
    "        \n",
    "        # Plot z = 0.5\n",
    "        plt.contour(u,v,z, levels = [0.5], colors=\"g\")\n",
    "\n",
    "plot_decision_boundary(w, b, X_mapped, y_train)\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Microchip Test 2') \n",
    "# Set the x-axis label\n",
    "plt.xlabel('Microchip Test 1') \n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "- one neuron\n",
    "- sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_dim=1,  activation = 'sigmoid', name='L1')\n",
    "])\n",
    "print(model.summary())\n",
    "\n",
    "logistic_layer = model.get_layer('L1')\n",
    "w,b = logistic_layer.get_weights()\n",
    "print(w,b)\n",
    "print(w.shape,b.shape)\n",
    "\n",
    "set_w = np.array([[2]])\n",
    "set_b = np.array([-4.5])\n",
    "logistic_layer.set_weights([set_w, set_b])\n",
    "print(logistic_layer.get_weights())\n",
    "\n",
    "a1 = model.predict(X_train[0].reshape(1,1))\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neural network(supervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural networks can learn non-linear relationships so you can opt to skip adding polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1234)  # to achieve consistent results\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(2,)), # expected shape of the input\n",
    "        tf.keras.layers.Dense(units=3, activation='relu', name='layer1'),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid', name='layer2')\n",
    "     ], name = 'classifier'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 3)                 9         \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13 (52.00 Byte)\n",
      "Trainable params: 13 (52.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$ matrix: $(u_{in}, u_{out})$  \n",
    "$b$ vector: ($u_{out},)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1(2, 3):\n",
      " [[-0.62252975  0.38249552 -0.13287777]\n",
      " [ 0.09647369  1.009989   -1.0684577 ]] \n",
      "b1(3,): [0. 0. 0.]\n",
      "W2(3, 1):\n",
      " [[ 1.1811253 ]\n",
      " [-0.50675476]\n",
      " [-0.5505224 ]] \n",
      "b2(1,): [0.]\n"
     ]
    }
   ],
   "source": [
    "# instantiated weights and biases\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights() # model.layers[0].get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights() # model.layers[1].get_weights()\n",
    "print(f\"W1{W1.shape}:\\n\", W1, f\"\\nb1{b1.shape}:\", b1)\n",
    "print(f\"W2{W2.shape}:\\n\", W2, f\"\\nb2{b2.shape}:\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function: 하나의 input data에 대해 target과 오차를 계산하는 함수  \n",
    "cost function: 모든 input data에 대해 losses를 계산하는 함수로 학습을 통해 이를 최소화 ex) MSE, Binary Cross Entropy, Cross Entropy\n",
    "\n",
    "$$ cost function = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} loss function$$  \n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} L(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE: \\frac{1}{m} \\sum_{i=0}^{m-1} \\frac{1}{2}(y_i - \\hat{y}_i)^2$$  \n",
    "$$Binary Cross Entropy: -\\frac{1}{m} \\sum_{i=0}^{m-1} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$  \n",
    "$$Cross Entropy: -\\sum_{i=0}^{m-1} y_i \\log(\\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient descent: $x \\rightarrow x - \\alpha \\nabla f(x)$ 를 통해 cost function을 최소화\n",
    "\n",
    "$$\\begin{align*}& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Gradient Descent\n",
    "- update parameters for entire training dataset\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "m = X.shape[1]  # Number of training examples\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Forward propagation\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    # Compute cost\n",
    "    cost_total = compute_cost(a, Y)  # Cost for m training examples\n",
    "    # Backward propagation\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    # Update parameters\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost\n",
    "    cost_avg = cost_total / m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch Gradient Descent\n",
    "- update parameters for n training examples\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "m = X.shape[1]  # Number of training examples\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    cost_total = 0\n",
    "    for (minibatch_X, minibatch_Y) in minibatches:\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(minibatch_X, parameters)   \n",
    "        # Compute cost\n",
    "        cost_total += compute_cost(a, minibatch_Y)\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost (optional)\n",
    "    cost_avg = cost_total / mini_batch_size\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "- update parameters for 1 training example\n",
    "- fast → used when training set is large\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "m = X.shape[1]  # Number of training examples\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    cost_total = 0\n",
    "    for j in range(0, m):\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Compute cost\n",
    "        cost_total += compute_cost(a, Y[:,j])  # Cost for one training example\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost\n",
    "    cost_avg = cost_total / m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate decay: redue learning rate over time to converge to the minimun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "model.compile(\n",
    "    # loss = 'mse' # regression\n",
    "    # loss = tf.keras.losses.BinaryCrossentropy(from_logits=True), # binary classification(0/1)\n",
    "    # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # multiclass classification(0~9)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent(efficiency broken into 32 batches)\n",
    "history = model.fit(\n",
    "    Xt,Yt,            \n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated weights and biases\n",
    "W1, b1 = model.get_layer(\"layer1\").get_weights()\n",
    "W2, b2 = model.get_layer(\"layer2\").get_weights()\n",
    "print(\"W1:\\n\", W1, \"\\nb1:\", b1)\n",
    "print(\"W2:\\n\", W2, \"\\nb2:\", b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track the progress of gradient descent\n",
    "def widgvis(fig):\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "\n",
    "def plot_loss_tf(history):\n",
    "    fig,ax = plt.subplots(1,1, figsize = (4,3))\n",
    "    widgvis(fig)\n",
    "    ax.plot(history.history['loss'], label='loss')\n",
    "    ax.set_ylim([0, 2])\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('loss (cost)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_tf(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "X_testn = norm_l(X_test)\n",
    "X_testn = tf.convert_to_tensor(X_testn)\n",
    "\n",
    "first_prediction = model.predict(X_testn[0].reshape(1,400)) \n",
    "print(f\"first prediction: {first_prediction}\")\n",
    "predictions = model.predict(X_testn)\n",
    "print(\"predictions = \\n\", predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification class conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_yhat = (predictions >= 0.5).astype(int) # binary classification(convert probabilites to decision)\n",
    "\n",
    "mc_yhat = tf.nn.softmax(predictions.numpy()) # multiclass classfication(if the desired output is probability)\n",
    "mc_yhat = np.argmax(predictions, axis=1) # multiclass classfication(if the desired output is category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cat_err(y, yhat):\n",
    "    \"\"\" \n",
    "    Calculate the categorization error\n",
    "    Args:\n",
    "      y    : (ndarray  Shape (m,) or (m,1))  target value of each example\n",
    "      yhat : (ndarray  Shape (m,) or (m,1))  predicted value of each example\n",
    "    Returns:|\n",
    "      err: (scalar)             \n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    incorrect = 0\n",
    "    for i in range(m):\n",
    "        if yhat[i] != y[i]:\n",
    "            incorrect += 1\n",
    "    err = incorrect/m\n",
    "    return(err)\n",
    "\n",
    "training_cerr_complex = eval_cat_err(y_train, model_predict(X_train))\n",
    "cv_cerr_complex = eval_cat_err(y_cv, model_predict(X_cv))\n",
    "print(f\"categorization error, training, complex model: {training_cerr_complex:0.3f}\")\n",
    "print(f\"categorization error, cv,       complex model: {cv_cerr_complex:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = lambda Xl: np.argmax(tf.nn.softmax(model.predict(Xl)).numpy(),axis=1)\n",
    "plt_nn(model_predict,X_train,y_train, classes, X_cv, y_cv, suptitle=\"Complex Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the training parameters at the end of each epoch  \n",
    "if it meets the threshold, stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    '''\n",
    "    Halts the training when the loss falls below 0.4\n",
    "\n",
    "    Args:\n",
    "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "      logs (dict) - metric results from the training epoch\n",
    "    '''\n",
    "\n",
    "    # Check the loss\n",
    "    if(logs.get('loss') < 0.4):\n",
    "\n",
    "      # Stop if threshold is met\n",
    "      print(\"\\nLoss is lower than 0.4 so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "# Instantiate class\n",
    "callbacks = myCallback()\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### He initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speed up gradient descent  \n",
    "avoid vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bias/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the baseline level of performance(human performance)  \n",
    "therefore, consider the gap between training error and cross validation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{just right: } J_{\\text{train}} \\downarrow, J_{\\text{cv}} \\downarrow$  \n",
    "\n",
    "$\\text{high bias problem/underfitting: } J_{\\text{train}} \\uparrow, J_{\\text{cv}} \\uparrow/{\\text{ training loss is far wrose than the baseline level of performance}}$\n",
    "* try adding polynomial features\n",
    "* try getting additional features\n",
    "* try decreasing the regularization parameter  \n",
    "\n",
    "$\\text{high variance problem/overfitting: } J_{\\text{train}} \\downarrow, J_{\\text{cv}} \\uparrow$\n",
    "* try increasing the regularization parameter\n",
    "* try smaller sets of features\n",
    "* get more training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 regularization\n",
    "* generalize to not learn complex patterns in data\n",
    "* encourages gradient descent to minimize $\\mathbf{w}$\n",
    "* used only in training\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} \\frac{1}{2}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2  + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{linear regression} $$ \n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m}  \\sum_{i=0}^{m-1} \\left[ -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m}  \\sum_{j=0}^{n-1} w_j^2 \\tag{logistic regression} $$\n",
    "$$\\begin{align*} \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)}  +  \\frac{\\lambda}{m} w_j \\\\ \\end{align*}$$\n",
    "$$\\begin{align*}&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}   \\; & \\text{for j := 0..n-1} \\\\ \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r = Sequential(\n",
    "    [\n",
    "        Dense(units=120, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "        Dense(units=40, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1)),\n",
    "        Dense(units=6, activation='linear'),\n",
    "    ], name= None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout\n",
    "* in each iteration randomly shut down some neruons for both forward and backward propatations\n",
    "* able to not rely on specific neurons\n",
    "* used only in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_models():\n",
    "    \n",
    "    tf.random.set_seed(20)\n",
    "    \n",
    "    model_1 = Sequential(\n",
    "        [\n",
    "            Dense(25, activation = 'relu'),\n",
    "            Dense(15, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_1'\n",
    "    )\n",
    "\n",
    "    model_2 = Sequential(\n",
    "        [\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(20, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_2'\n",
    "    )\n",
    "\n",
    "    model_3 = Sequential(\n",
    "        [\n",
    "            Dense(32, activation = 'relu'),\n",
    "            Dense(16, activation = 'relu'),\n",
    "            Dense(8, activation = 'relu'),\n",
    "            Dense(4, activation = 'relu'),\n",
    "            Dense(12, activation = 'relu'),\n",
    "            Dense(1, activation = 'linear')\n",
    "        ],\n",
    "        name='model_3'\n",
    "    )\n",
    "    \n",
    "    model_list = [model_1, model_2, model_3]\n",
    "    \n",
    "    return model_list\n",
    "\n",
    "\n",
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_mses = []\n",
    "nn_cv_mses = []\n",
    "\n",
    "# Build the models\n",
    "nn_models = build_models()\n",
    "\n",
    "# Loop over the the models\n",
    "for model in nn_models:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        X_train_mapped_scaled, y_train,\n",
    "        epochs=300,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "\n",
    "    \n",
    "    # Record the training MSEs\n",
    "    yhat = model.predict(X_train_mapped_scaled)\n",
    "    train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "    nn_train_mses.append(train_mse)\n",
    "    \n",
    "    # Record the cross validation MSEs \n",
    "    yhat = model.predict(X_cv_mapped_scaled)\n",
    "    cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "    nn_cv_mses.append(cv_mse)\n",
    "\n",
    "    \n",
    "# print results\n",
    "print(\"RESULTS:\")\n",
    "for model_num in range(len(nn_train_mses)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training MSE: {nn_train_mses[model_num]:.2f}, \" +\n",
    "        f\"CV MSE: {nn_cv_mses[model_num]:.2f}\"\n",
    "        )\n",
    "    \n",
    "# Select the model with the lowest CV MSE\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test MSE\n",
    "yhat = nn_models[model_num-1].predict(X_test_mapped_scaled)\n",
    "test_mse = mean_squared_error(y_test, yhat) / 2\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training MSE: {nn_train_mses[model_num-1]:.2f}\")\n",
    "print(f\"Cross Validation MSE: {nn_cv_mses[model_num-1]:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification models(error metric: fraction of the data that the model has misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_error = []\n",
    "nn_cv_error = []\n",
    "\n",
    "# Build the models\n",
    "models_bc = utils.build_models()\n",
    "\n",
    "# Loop over each model\n",
    "for model in models_bc:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_bc_train_scaled, y_bc_train,\n",
    "        epochs=200,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "    \n",
    "    # Set the threshold for classification\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Record the fraction of misclassified examples for the training set\n",
    "    yhat = model.predict(x_bc_train_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    train_error = np.mean(yhat != y_bc_train)\n",
    "    nn_train_error.append(train_error)\n",
    "\n",
    "    # Record the fraction of misclassified examples for the cross validation set\n",
    "    yhat = model.predict(x_bc_cv_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    cv_error = np.mean(yhat != y_bc_cv)\n",
    "    nn_cv_error.append(cv_error)\n",
    "\n",
    "# Print the result\n",
    "for model_num in range(len(nn_train_error)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
    "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest error\n",
    "model_num = 3\n",
    "\n",
    "# Compute the test error\n",
    "yhat = models_bc[model_num-1].predict(x_bc_test_scaled)\n",
    "yhat = tf.math.sigmoid(yhat)\n",
    "yhat = np.where(yhat >= threshold, 1, 0)\n",
    "nn_test_error = np.mean(yhat != y_bc_test)\n",
    "\n",
    "print(f\"Selected Model: {model_num}\")\n",
    "print(f\"Training Set Classification Error: {nn_train_error[model_num-1]:.4f}\")\n",
    "print(f\"CV Set Classification Error: {nn_cv_error[model_num-1]:.4f}\")\n",
    "print(f\"Test Set Classification Error: {nn_test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 1, baseline=None):\n",
    "    \n",
    "    train_mses = []\n",
    "    cv_mses = []\n",
    "    models = []\n",
    "    scalers = []\n",
    "\n",
    "    # Loop over 10 times. Each adding one more degree of polynomial higher than the last.\n",
    "    for reg_param in reg_params:\n",
    "\n",
    "        # Add polynomial features to the training set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_train_mapped = poly.fit_transform(x_train)\n",
    "\n",
    "        # Scale the training set\n",
    "        scaler_poly = StandardScaler()\n",
    "        X_train_mapped_scaled = scaler_poly.fit_transform(X_train_mapped)\n",
    "        scalers.append(scaler_poly)\n",
    "\n",
    "        # Create and train the model\n",
    "        model = Ridge(alpha=reg_param)\n",
    "        model.fit(X_train_mapped_scaled, y_train)\n",
    "        models.append(model)\n",
    "\n",
    "        # Compute the training MSE\n",
    "        yhat = model.predict(X_train_mapped_scaled)\n",
    "        train_mse = mean_squared_error(y_train, yhat) / 2\n",
    "        train_mses.append(train_mse)\n",
    "\n",
    "        # Add polynomial features and scale the cross-validation set\n",
    "        poly = PolynomialFeatures(degree, include_bias=False)\n",
    "        X_cv_mapped = poly.fit_transform(x_cv)\n",
    "        X_cv_mapped_scaled = scaler_poly.transform(X_cv_mapped)\n",
    "\n",
    "        # Compute the cross-validation MSE\n",
    "        yhat = model.predict(X_cv_mapped_scaled)\n",
    "        cv_mse = mean_squared_error(y_cv, yhat) / 2\n",
    "        cv_mses.append(cv_mse)\n",
    "\n",
    "    # Plot the results\n",
    "    reg_params = [str(x) for x in reg_params]\n",
    "    plt.plot(reg_params, train_mses, marker='o', c='r', label='training MSEs'); \n",
    "    plt.plot(reg_params, cv_mses, marker='o', c='b', label='CV MSEs'); \n",
    "    plt.plot(reg_params, np.repeat(baseline, len(reg_params)), linestyle='--', label='baseline')\n",
    "    plt.title(\"lambda vs. train and CV MSEs\")\n",
    "    plt.xlabel(\"lambda\"); \n",
    "    plt.ylabel(\"MSE\"); \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define lambdas to plot\n",
    "reg_params = [10, 5, 2, 1, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01]\n",
    "\n",
    "# Define degree of polynomial and train for each value of lambda\n",
    "train_plot_reg_params(reg_params, x_train, y_train, x_cv, y_cv, degree= 4, baseline=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_range = np.array([0.0, 1e-6, 1e-5, 1e-4,1e-3,1e-2, 1e-1,1,10,100])\n",
    "num_steps = len(lambda_range)\n",
    "degree = 10\n",
    "err_train = np.zeros(num_steps)    \n",
    "err_cv = np.zeros(num_steps)       \n",
    "x = np.linspace(0,int(X.max()),100) \n",
    "y_pred = np.zeros((100,num_steps))  #columns are lines to plot\n",
    "\n",
    "for i in range(num_steps):\n",
    "    lambda_= lambda_range[i]\n",
    "    lmodel = lin_model(degree, regularization=True, lambda_=lambda_)\n",
    "    lmodel.fit(X_train, y_train)\n",
    "    yhat = lmodel.predict(X_train)\n",
    "    err_train[i] = lmodel.mse(y_train, yhat)\n",
    "    yhat = lmodel.predict(X_cv)\n",
    "    err_cv[i] = lmodel.mse(y_cv, yhat)\n",
    "    y_pred[:,i] = lmodel.predict(x)\n",
    "    \n",
    "optimal_reg_idx = np.argmin(err_cv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "models=[None] * len(lambdas)\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    lambda_ = lambdas[i]\n",
    "    models[i] =  Sequential(\n",
    "        [\n",
    "            Dense(120, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "            Dense(40, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "            Dense(classes, activation = 'linear')\n",
    "        ]\n",
    "    )\n",
    "    models[i].compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "    )\n",
    "\n",
    "    models[i].fit(\n",
    "        X_train,y_train,\n",
    "        epochs=1000\n",
    "    )\n",
    "    print(f\"Finished lambda = {lambda_}\")\n",
    "\n",
    "def plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv):\n",
    "    err_train = np.zeros(len(lambdas))\n",
    "    err_cv = np.zeros(len(lambdas))\n",
    "    for i in range(len(models)):\n",
    "        err_train[i] = eval_cat_err(y_train,np.argmax( models[i](X_train), axis=1))\n",
    "        err_cv[i] = eval_cat_err(y_cv, np.argmax( models[i](X_cv), axis=1))\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "    fig.canvas.toolbar_visible = False\n",
    "    fig.canvas.header_visible = False\n",
    "    fig.canvas.footer_visible = False\n",
    "    ax.set_title(\"error vs regularization\",fontsize = 12)\n",
    "    ax.plot(lambdas, err_train, marker='o', label=\"train error\", color = dlc[\"dlblue\"])\n",
    "    ax.plot(lambdas, err_cv,    marker='o', label=\"cv error\",    color = dlc[\"dlorange\"])\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylim(*ax.get_ylim())\n",
    "    ax.set_xlabel(\"Regularization (lambda)\",fontsize = 14)\n",
    "    ax.set_ylabel(\"Error\",fontsize = 14)\n",
    "    ax.legend()\n",
    "    fig.suptitle(\"Tuning Regularization\",fontsize = 14)\n",
    "    ax.text(0.05,0.14,\"Training Error\\nlower than CV\",fontsize=12, ha='left',transform=ax.transAxes,color = dlc[\"dlblue\"])\n",
    "    ax.text(0.95,0.14,\"Similar\\nTraining, CV\",    fontsize=12, ha='right',transform=ax.transAxes,color = dlc[\"dlblue\"])\n",
    "    plt.show()\n",
    "\n",
    "plot_iterate(lambdas, models, X_train, y_train, X_cv, y_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neural network(unsupervised learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train from normal examples and filter examples that aren't normal \n",
    "* anomaly detection vs supervised learning 1\n",
    "    1. anomaly detection: capable of finding previously unseen defects\n",
    "    2. supervised learning: capable of finding previously seen defects\n",
    "* anomaly detection vs supervised learning 2\n",
    "    1. anomaly detection: needs to choose what features to use\n",
    "    2. supervised learning: don't need to choose what features to use (able to learn which features to not use from labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. draw histogram for each features\n",
    "    $$p(x_i ; \\mu_i,\\sigma_i ^2) \\rightarrow \\mathbf{plt.hist(x)}$$\n",
    "2. make non-gaussian features more like gaussian (don't forget to apply to cross validation, test set)\n",
    "    $$log(x_i)$$\n",
    "    $$log(x_i + C)$$\n",
    "    $$\\sqrt[2]{x_i}$$\n",
    "    $$\\sqrt[3]{x_i}$$\n",
    "2. estimate Gaussian distribution from features ($x_i$ &rightarrow; $\\mu_i$, $\\sigma_i^2$)\n",
    "$$\\mu_i = \\frac{1}{m} \\sum_{j=1}^m x_i^{(j)}$$\n",
    "$$\\sigma_i^2 = \\frac{1}{m} \\sum_{j=1}^m (x_i^{(j)} - \\mu_i)^2$$\n",
    "$$p(x_i ; \\mu_i,\\sigma_i ^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma ^2}}\\exp^{ - \\frac{(x_i - \\mu_i)^2}{2 \\sigma_i ^2} }$$\n",
    "3. identify anomalous example\n",
    "    * want $p(x)$ large for normal examples and small for anomalous examples\n",
    "    * if $p(x) is comparable(large) for normal and anomalous examples, add more features to achieve above\n",
    "$$p(x) = \\prod_{i=1}^{n} p(x_i; \\mu_i, \\sigma_i^2)$$\n",
    "$$if\\ anomaly,\\ p(x) < \\epsilon\\rightarrow y = 1$$\n",
    "4. select best $\\epsilon$ using F1 score\n",
    "$$F_1 = \\frac{2\\cdot prec \\cdot rec}{prec + rec}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate mean and variance of each feature\n",
    "def estimate_gaussian(X): \n",
    "    \"\"\"\n",
    "    Calculates mean and variance of all features \n",
    "    in the dataset\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Data matrix\n",
    "    \n",
    "    Returns:\n",
    "        mu (ndarray): (n,) Mean of all features\n",
    "        var (ndarray): (n,) Variance of all features\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    mu = 1/m * np.sum(X, axis=0)\n",
    "    var = 1/m * np.sum((X-mu)**2, axis=0)\n",
    "        \n",
    "    return mu, var\n",
    "\n",
    "mu, var = estimate_gaussian(X_train)              \n",
    "\n",
    "print(\"Mean of each feature:\", mu)\n",
    "print(\"Variance of each feature:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the probabilites for the training set\n",
    "def multivariate_gaussian(X, mu, var):\n",
    "    \"\"\"\n",
    "    Computes the probability \n",
    "    density function of the examples X under the multivariate gaussian \n",
    "    distribution with parameters mu and var. If var is a matrix, it is\n",
    "    treated as the covariance matrix. If var is a vector, it is treated\n",
    "    as the var values of the variances in each dimension (a diagonal\n",
    "    covariance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    k = len(mu)\n",
    "    \n",
    "    if var.ndim == 1:\n",
    "        var = np.diag(var)\n",
    "        \n",
    "    X = X - mu\n",
    "    p = (2* np.pi)**(-k/2) * np.linalg.det(var)**(-0.5) * \\\n",
    "        np.exp(-0.5 * np.sum(np.matmul(X, np.linalg.pinv(var)) * X, axis=1))\n",
    "    \n",
    "    return p\n",
    "\n",
    "p_train = multivariate_gaussian(X_train, mu, var)\n",
    "p_val = multivariate_gaussian(X_val, mu, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best threshold\n",
    "def select_threshold(y_val, p_val): \n",
    "    \"\"\"\n",
    "    Finds the best threshold to use for selecting outliers \n",
    "    based on the results from a validation set (p_val) \n",
    "    and the ground truth (y_val)\n",
    "    \n",
    "    Args:\n",
    "        y_val (ndarray): Ground truth on validation set\n",
    "        p_val (ndarray): Results on validation set\n",
    "        \n",
    "    Returns:\n",
    "        epsilon (float): Threshold chosen \n",
    "        F1 (float):      F1 score by choosing epsilon as threshold\n",
    "    \"\"\" \n",
    "\n",
    "    best_epsilon = 0\n",
    "    best_F1 = 0\n",
    "    F1 = 0\n",
    "    \n",
    "    step_size = (max(p_val) - min(p_val)) / 1000\n",
    "    \n",
    "    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n",
    "        \n",
    "        predictions = p_val < epsilon\n",
    "\n",
    "        tp = np.sum((predictions == 1) & (y_val == 1))\n",
    "        fp = np.sum((predictions == 1) & (y_val == 0))\n",
    "        fn = np.sum((predictions == 0) & (y_val == 1))\n",
    "\n",
    "        prec = tp / (tp + fp)\n",
    "        rec = tp / (tp + fn)\n",
    "\n",
    "        F1 = 2 * prec * rec / (prec + rec)\n",
    "        \n",
    "        if F1 > best_F1:\n",
    "            best_F1 = F1\n",
    "            best_epsilon = epsilon\n",
    "        \n",
    "    return best_epsilon, best_F1\n",
    "\n",
    "epsilon, F1 = select_threshold(y_val, p_val)\n",
    "\n",
    "print('Best epsilon found using cross-validation: %e' % epsilon)\n",
    "print('Best F1 on Cross Validation Set: %f' % F1)\n",
    "print('# Anomalies found: %d'% sum(p_val < epsilon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
