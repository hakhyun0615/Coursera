{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FaceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- face verification(1:1 matching problem)\n",
    "    - if the distance between two encoded images is below a chosen threshold, it is same person  \n",
    "    <img src=\"../images/face verification.png\" style=\"width:380px;height:150px;\">\n",
    "    - triplet loss function\n",
    "        - $(A^{(i)}, P^{(i)}, N^{(i)})$:\n",
    "            - A is an \"Anchor\" image--a picture of a person.\n",
    "            - P is a \"Positive\" image--a picture of the same person as the Anchor image.\n",
    "            - N is a \"Negative\" image--a picture of a different person than the Anchor image.\n",
    "        - $\\mathcal{L}(A^{(i)}, P^{(i)}, N^{(i)}) = max\\ (\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2 + \\alpha,\\ 0)$  \n",
    "        $\\mathcal{J} = \\sum^{m}_{i=1} \\mathcal{L}(A^{(i)}, P^{(i)}, N^{(i)})$\n",
    "            - push $A,P$ together and pull $A,N$ apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_encoding(image_path, model):\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(160, 160))\n",
    "    img = np.around(np.array(img) / 255.0, decimals=12)\n",
    "    x_train = np.expand_dims(img, axis=0)\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    return embedding / np.linalg.norm(embedding, ord=2)\n",
    "\n",
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
    "    \n",
    "    Arguments:\n",
    "        image_path -- path to an image\n",
    "        identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n",
    "        database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "        model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "        dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "        door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    # Step 2: Compute distance with identity's image (≈ 1 line)\n",
    "    dist = np.linalg.norm(encoding - database[identity])\n",
    "    # Step 3: Open the door if dist < 0.7, else don't open (≈ 3 lines)\n",
    "    if dist < 0.7:\n",
    "        print(\"It's \" + str(identity) + \", welcome in!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"It's not \" + str(identity) + \", please go away\")\n",
    "        door_open = False\n",
    "         \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- face recognition(1:k matching problem)\n",
    "    1. find the encoding from the database that has smallest distance with the target encoding\n",
    "    2. if the distance between two encoded images is below a chosen threshold, it is same person  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the office by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "        image_path -- path to an image\n",
    "        database -- database containing image encodings along with the name of the person on the image\n",
    "        model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "        min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "        identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n",
    "    encoding =  img_to_encoding(image_path, model)\n",
    "    \n",
    "    ## Step 2: Find the closest encoding ##\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current db_enc from the database. (≈ 1 line)\n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
